import torch
from PIL import Image, ImageDraw
import numpy as np
import os
import json
import re
import copy
import supervision as sv
from typing import Tuple, Optional, Any, List

try:
    from florence_sam_processor import process_image
    from utils.sam import model_to_config_map as sam_model_to_config_map
    from utils.sam import load_sam_image_model, run_sam_inference
    from utils.modes import IMAGE_INFERENCE_MODES, IMAGE_OPEN_VOCABULARY_DETECTION_MODE, IMAGE_CAPTION_GROUNDING_MASKS_MODE, VIDEO_INFERENCE_MODES
    from mask_cleaner import remove_small_regions
except ImportError:
    # We're running as a module
    from .florence_sam_processor import process_image
    from .utils.sam import model_to_config_map as sam_model_to_config_map
    from .utils.sam import load_sam_image_model, run_sam_inference
    from .utils.modes import IMAGE_INFERENCE_MODES, IMAGE_OPEN_VOCABULARY_DETECTION_MODE, IMAGE_CAPTION_GROUNDING_MASKS_MODE, VIDEO_INFERENCE_MODES
    from .mask_cleaner import remove_small_regions

# GroundingDINO imports
import logging
import comfy.model_management

# Import GroundingDINO loader
try:
    from grounding_dino_loader import groundingdino_model_list
    from local_groundingdino.datasets import transforms as T
except ImportError:
    try:
        from .grounding_dino_loader import groundingdino_model_list
        from local_groundingdino.datasets import transforms as T
    except ImportError:
        print("Warning: GroundingDINO dependencies not found. Please install them.")
        T = None
        groundingdino_model_list = {}

logger = logging.getLogger('vvl_GroundingDinoSAM2')

# Format conversion helpers
def tensor2pil(t_image: torch.Tensor) -> Image.Image:
    return Image.fromarray(np.clip(255.0 * t_image.cpu().numpy(), 0, 255).astype(np.uint8))

def pil2tensor(image: Image.Image) -> torch.Tensor:
    return torch.from_numpy(np.array(image).astype(np.float32) / 255.0)

# GroundingDINO prediction function

def groundingdino_predict(dino_model, image, prompt, threshold):
    def load_dino_image(image_pil):
        transform = T.Compose([
            T.RandomResize([800], max_size=1333),
            T.ToTensor(),
            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])
        image, _ = transform(image_pil, None)  # 3, h, w
        return image

    def get_grounding_output(model, image, caption, box_threshold):
        caption = caption.lower()
        caption = caption.strip()
        if not caption.endswith("."):
            caption = caption + "."
        device = comfy.model_management.get_torch_device()
        image = image.to(device)
        with torch.no_grad():
            outputs = model(image[None], captions=[caption])
        logits = outputs["pred_logits"].sigmoid()[0]  # (nq, 256)
        boxes = outputs["pred_boxes"][0]  # (nq, 4)
        # filter output
        logits_filt = logits.clone()
        boxes_filt = boxes.clone()
        filt_mask = logits_filt.max(dim=1)[0] > box_threshold
        logits_filt = logits_filt[filt_mask]  # num_filt, 256
        boxes_filt = boxes_filt[filt_mask]  # num_filt, 4
        return boxes_filt.cpu()

    dino_image = load_dino_image(image.convert("RGB"))
    boxes_filt = get_grounding_output(dino_model, dino_image, prompt, threshold)
    H, W = image.size[1], image.size[0]
    for i in range(boxes_filt.size(0)):
        boxes_filt[i] = boxes_filt[i] * torch.Tensor([W, H, W, H])
        boxes_filt[i][:2] -= boxes_filt[i][2:] / 2
        boxes_filt[i][2:] += boxes_filt[i][:2]
    return boxes_filt

# Color palette and annotators for image annotation
COLORS = ['#FF1493', '#00BFFF', '#FF6347', '#FFD700', '#32CD32', '#8A2BE2']
COLOR_PALETTE = sv.ColorPalette.from_hex(COLORS)

# å…¼å®¹ä¸åŒç‰ˆæœ¬çš„supervision
try:
    # æ–°ç‰ˆæœ¬ supervision (>= 0.21.0)
    BOX_ANNOTATOR = sv.BoxAnnotator(color=COLOR_PALETTE, color_lookup=sv.ColorLookup.INDEX)
    LABEL_ANNOTATOR = sv.LabelAnnotator(
        color=COLOR_PALETTE,
        color_lookup=sv.ColorLookup.INDEX,
        text_position=sv.Position.CENTER_OF_MASS,
        text_color=sv.Color.from_hex("#000000"),
        border_radius=5
    )
    MASK_ANNOTATOR = sv.MaskAnnotator(
        color=COLOR_PALETTE,
        color_lookup=sv.ColorLookup.INDEX
    )
except AttributeError:
    # æ—§ç‰ˆæœ¬ supervision (0.6.0)
    BOX_ANNOTATOR = sv.BoxAnnotator(color=COLOR_PALETTE)
    MASK_ANNOTATOR = sv.MaskAnnotator(color=COLOR_PALETTE)
    
    # supervision 0.6.0 æ²¡æœ‰ LabelAnnotatorï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„æ›¿ä»£å“
    class SimpleLabelAnnotator:
        def __init__(self, color=None, **kwargs):
            self.color = color
            
        def annotate(self, scene, detections, labels=None):
            # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œç›´æ¥è¿”å›åŸå›¾åƒ
            if not labels or len(labels) == 0:
                return scene
            
            import cv2
            output = scene.copy()
            
            for i, (box, label) in enumerate(zip(detections.xyxy, labels)):
                x1, y1, x2, y2 = map(int, box)
                # åœ¨æ¡†çš„å·¦ä¸Šè§’ç»˜åˆ¶æ ‡ç­¾
                cv2.putText(output, str(label), (x1, y1-10), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)
            
            return output
    
    LABEL_ANNOTATOR = SimpleLabelAnnotator(color=COLOR_PALETTE)

def annotate_image(image, detections):
    output_image = image.copy()
    output_image = MASK_ANNOTATOR.annotate(output_image, detections)
    output_image = BOX_ANNOTATOR.annotate(output_image, detections)
    output_image = LABEL_ANNOTATOR.annotate(output_image, detections)
    return output_image

def resolve_duplicate_names(object_names):
    """
    è§£å†³å¯¹è±¡åç§°é‡å¤é—®é¢˜ï¼Œä¸ºé‡å¤çš„åç§°è‡ªåŠ¨æ·»åŠ æ•°å­—åç¼€
    ä¾‹å¦‚ï¼š['person', 'person', 'car', 'person'] -> ['person', 'person_2', 'car', 'person_3']
    
    Args:
        object_names: åŸå§‹å¯¹è±¡åç§°åˆ—è¡¨
    
    Returns:
        resolved_names: è§£å†³é‡å¤åçš„å¯¹è±¡åç§°åˆ—è¡¨
    """
    if not object_names:
        return object_names
    
    name_counts = {}
    resolved_names = []
    
    for name in object_names:
        # æ¸…ç†åç§°ï¼Œå»é™¤å¯èƒ½å·²å­˜åœ¨çš„æ•°å­—åç¼€
        base_name = name
        if '_' in name:
            parts = name.split('_')
            if len(parts) >= 2 and parts[-1].isdigit():
                base_name = '_'.join(parts[:-1])
        
        if base_name in name_counts:
            name_counts[base_name] += 1
            resolved_name = f"{base_name}_{name_counts[base_name]}"
        else:
            name_counts[base_name] = 1
            resolved_name = base_name
        
        resolved_names.append(resolved_name)
    
    return resolved_names

def verify_data_consistency(object_names, detections_with_masks, output_masks, image_index=0):
    """
    éªŒè¯å¯¹è±¡åç§°ã€æ£€æµ‹ç»“æœå’Œmaskä¹‹é—´çš„æ•°æ®ä¸€è‡´æ€§
    
    Args:
        object_names: å¯¹è±¡åç§°åˆ—è¡¨
        detections_with_masks: supervision.Detectionså¯¹è±¡
        output_masks: mask tensoråˆ—è¡¨
        image_index: å›¾åƒç´¢å¼•ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    
    Returns:
        bool: æ˜¯å¦ä¸€è‡´
    """
    issues = []
    
    # æ£€æŸ¥åŸºæœ¬æ•°é‡
    name_count = len(object_names) if object_names else 0
    
    if detections_with_masks is not None:
        if hasattr(detections_with_masks, 'xyxy') and detections_with_masks.xyxy is not None:
            bbox_count = len(detections_with_masks.xyxy)
        else:
            bbox_count = 0
            
        if hasattr(detections_with_masks, 'mask') and detections_with_masks.mask is not None:
            detection_mask_count = len(detections_with_masks.mask)
        else:
            detection_mask_count = 0
    else:
        bbox_count = 0
        detection_mask_count = 0
    
    output_mask_count = len(output_masks) if output_masks else 0
    
    # éªŒè¯æ•°é‡ä¸€è‡´æ€§
    if name_count != bbox_count:
        issues.append(f"å¯¹è±¡åç§°æ•°é‡({name_count})ä¸bboxæ•°é‡({bbox_count})ä¸åŒ¹é…")
    
    if name_count != output_mask_count:
        issues.append(f"å¯¹è±¡åç§°æ•°é‡({name_count})ä¸è¾“å‡ºmaskæ•°é‡({output_mask_count})ä¸åŒ¹é…")
        
    if bbox_count != detection_mask_count:
        issues.append(f"bboxæ•°é‡({bbox_count})ä¸æ£€æµ‹maskæ•°é‡({detection_mask_count})ä¸åŒ¹é…")
    
    # æ‰“å°éªŒè¯ç»“æœ
    if issues:
        print(f"âŒ VVL_GroundingDinoSAM2: Image {image_index} - æ•°æ®ä¸€è‡´æ€§éªŒè¯å¤±è´¥:")
        for issue in issues:
            print(f"   - {issue}")
        print(f"   å¯¹è±¡åç§°: {object_names}")
        return False
    else:
        print(f"âœ… VVL_GroundingDinoSAM2: Image {image_index} - æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
        print(f"   æ€»æ•°: {name_count}ä¸ªå¯¹è±¡")
        print(f"   å¯¹è±¡åç§°: {object_names}")
        return True

def calculate_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoU (Intersection over Union)"""
    # boxæ ¼å¼: [x1, y1, x2, y2]
    x1_inter = max(box1[0], box2[0])
    y1_inter = max(box1[1], box2[1])
    x2_inter = min(box1[2], box2[2])
    y2_inter = min(box1[3], box2[3])
    
    # è®¡ç®—äº¤é›†é¢ç§¯
    if x2_inter <= x1_inter or y2_inter <= y1_inter:
        return 0.0
    
    inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
    
    # è®¡ç®—å¹¶é›†é¢ç§¯
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union_area = box1_area + box2_area - inter_area
    
    if union_area <= 0:
        return 0.0
    
    return inter_area / union_area

def remove_duplicate_boxes(boxes, object_names, iou_threshold=0.5):
    """ä½¿ç”¨NMSç®—æ³•å»é™¤é‡å¤çš„è¾¹ç•Œæ¡†ï¼ˆä»…ä½œä¸ºåˆæ­¥è¿‡æ»¤ï¼‰"""
    if boxes.shape[0] == 0:
        return boxes, object_names
    
    # è®¡ç®—æ¯ä¸ªæ¡†çš„é¢ç§¯
    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
    
    # æŒ‰é¢ç§¯æ’åºï¼ˆä¿ç•™è¾ƒå¤§çš„æ¡†ï¼‰
    order = torch.argsort(areas, descending=True)
    
    keep_indices = []
    remaining = order.tolist()
    
    while remaining:
        # å–å‡ºå½“å‰æœ€å¤§é¢ç§¯çš„æ¡†
        current_idx = remaining[0]
        keep_indices.append(current_idx)
        remaining.remove(current_idx)
        
        if not remaining:
            break
        
        # è®¡ç®—å½“å‰æ¡†ä¸å…¶ä½™æ¡†çš„IoU
        current_box = boxes[current_idx]
        to_remove = []
        
        for other_idx in remaining:
            other_box = boxes[other_idx]
            iou = calculate_iou(current_box, other_box)
            
            if iou > iou_threshold:
                to_remove.append(other_idx)
        
        # ç§»é™¤é‡å çš„æ¡†
        for idx in to_remove:
            remaining.remove(idx)
    
    # è¿”å›å»é‡åçš„boxeså’Œå¯¹åº”çš„object_names
    keep_indices = sorted(keep_indices)
    filtered_boxes = boxes[keep_indices]
    filtered_object_names = [object_names[i] for i in keep_indices] if object_names else []
    
    return filtered_boxes, filtered_object_names

def calculate_mask_containment_ratio(mask1, mask2):
    """è®¡ç®— mask2 è¢« mask1 åŒ…å«çš„æ¯”ä¾‹"""
    # ç¡®ä¿ä¸¤ä¸ªmaskçš„å½¢çŠ¶ç›¸åŒ
    if mask1.shape != mask2.shape:
        return 0.0
    
    # è½¬æ¢ä¸ºå¸ƒå°”æ•°ç»„
    mask1_bool = mask1 > 0.5
    mask2_bool = mask2 > 0.5
    
    # è®¡ç®— mask2 çš„é¢ç§¯
    mask2_area = np.sum(mask2_bool)
    if mask2_area == 0:
        return 0.0
    
    # è®¡ç®— mask2 è¢« mask1 åŒ…å«çš„åŒºåŸŸ
    intersection = np.sum(mask1_bool & mask2_bool)
    
    # è¿”å›åŒ…å«æ¯”ä¾‹
    return intersection / mask2_area

def remove_duplicate_masks_by_containment(output_images, output_masks, detections_with_masks, object_names, containment_threshold=0.8):
    """åŸºäºå®é™…maskå½¢çŠ¶å»é™¤è¢«åŒ…å«çš„é‡å¤åˆ†å‰²ç»“æœ"""
    if not output_masks or len(output_masks) <= 1:
        return output_images, output_masks, detections_with_masks, object_names
    
    # è½¬æ¢æ‰€æœ‰maskä¸ºnumpyæ•°ç»„ç”¨äºè®¡ç®—
    masks_np = []
    for mask_tensor in output_masks:
        if len(mask_tensor.shape) == 3:
            # å¦‚æœæ˜¯3D tensor (H, W, C)ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
            mask_np = mask_tensor[:, :, 0].numpy()
        else:
            # å¦‚æœæ˜¯2D tensor (H, W)
            mask_np = mask_tensor.numpy()
        masks_np.append(mask_np)
    
    # è®¡ç®—æ¯ä¸ªmaskçš„é¢ç§¯ï¼ŒæŒ‰é¢ç§¯æ’åºï¼ˆä¿ç•™å¤§çš„ï¼‰
    mask_areas = [np.sum(mask > 0.5) for mask in masks_np]
    sorted_indices = sorted(range(len(mask_areas)), key=lambda i: mask_areas[i], reverse=True)
    
    keep_indices = []
    
    for i in sorted_indices:
        current_mask = masks_np[i]
        should_keep = True
        
        # æ£€æŸ¥å½“å‰maskæ˜¯å¦è¢«å·²ä¿ç•™çš„ä»»ä½•maskåŒ…å«
        for kept_idx in keep_indices:
            kept_mask = masks_np[kept_idx]
            containment_ratio = calculate_mask_containment_ratio(kept_mask, current_mask)
            
            if containment_ratio > containment_threshold:
                # å½“å‰maskè¢«åŒ…å«ç¨‹åº¦è¶…è¿‡é˜ˆå€¼ï¼Œä¸ä¿ç•™
                obj_name = object_names[i] if i < len(object_names) else f"object_{i+1}"
                kept_obj_name = object_names[kept_idx] if kept_idx < len(object_names) else f"object_{kept_idx+1}"
                print(f"VVL_GroundingDinoSAM2: ç§»é™¤è¢«åŒ…å«çš„mask - '{obj_name}'è¢«'{kept_obj_name}'åŒ…å«{containment_ratio:.2%}")
                should_keep = False
                break
        
        if should_keep:
            keep_indices.append(i)
    
    # æŒ‰åŸå§‹é¡ºåºæ’åºä¿ç•™çš„ç´¢å¼•
    keep_indices = sorted(keep_indices)
    
    if len(keep_indices) == len(output_masks):
        # æ²¡æœ‰éœ€è¦ç§»é™¤çš„
        return output_images, output_masks, detections_with_masks, object_names
    
    # è¿‡æ»¤ç»“æœ
    filtered_output_images = [output_images[i] for i in keep_indices] if output_images else []
    filtered_output_masks = [output_masks[i] for i in keep_indices]
    filtered_object_names = [object_names[i] for i in keep_indices] if object_names else []
    
    # è¿‡æ»¤detections_with_masks
    if detections_with_masks is not None:
        # åˆ›å»ºæ–°çš„detectionså¯¹è±¡ï¼ŒåªåŒ…å«ä¿ç•™çš„ç´¢å¼•
        if hasattr(detections_with_masks, 'xyxy') and detections_with_masks.xyxy is not None:
            filtered_xyxy = detections_with_masks.xyxy[keep_indices]
        else:
            filtered_xyxy = None
            
        if hasattr(detections_with_masks, 'mask') and detections_with_masks.mask is not None:
            filtered_mask = detections_with_masks.mask[keep_indices]
        else:
            filtered_mask = None
            
        if hasattr(detections_with_masks, 'confidence') and detections_with_masks.confidence is not None:
            filtered_confidence = detections_with_masks.confidence[keep_indices]
        else:
            filtered_confidence = None
            
        if hasattr(detections_with_masks, 'class_id') and detections_with_masks.class_id is not None:
            filtered_class_id = detections_with_masks.class_id[keep_indices]
        else:
            filtered_class_id = None
        
        # åˆ›å»ºæ–°çš„Detectionså¯¹è±¡
        filtered_detections = sv.Detections(
            xyxy=filtered_xyxy,
            mask=filtered_mask,
            confidence=filtered_confidence,
            class_id=filtered_class_id
        )
        
        # å¤åˆ¶dataå­—å…¸
        if hasattr(detections_with_masks, 'data') and detections_with_masks.data:
            filtered_detections.data = {}
            for key, value in detections_with_masks.data.items():
                if isinstance(value, (list, np.ndarray)) and len(value) == len(output_masks):
                    if isinstance(value, list):
                        filtered_detections.data[key] = [value[i] for i in keep_indices]
                    else:
                        filtered_detections.data[key] = value[keep_indices]
                else:
                    filtered_detections.data[key] = value
    else:
        filtered_detections = None
    
    print(f"VVL_GroundingDinoSAM2: åŸºäºmaskåŒ…å«å…³ç³»è¿‡æ»¤æ‰ {len(output_masks) - len(keep_indices)} ä¸ªé‡å¤åˆ†å‰²ç»“æœ")
    
    return filtered_output_images, filtered_output_masks, filtered_detections, filtered_object_names

def filter_by_area(output_images, output_masks, detections_with_masks, object_names, image_size, 
                  min_area_ratio=0.0001, max_area_ratio=0.9):
    """
    æ ¹æ®maské¢ç§¯å¤§å°è¿‡æ»¤åˆ†å‰²ç»“æœ
    
    Args:
        output_images: SAM2åˆ†å‰²å¾—åˆ°çš„é®ç½©å›¾åƒåˆ—è¡¨
        output_masks: SAM2åˆ†å‰²å¾—åˆ°çš„maskåˆ—è¡¨  
        detections_with_masks: supervision.Detectionså¯¹è±¡
        object_names: å¯¹è±¡åç§°åˆ—è¡¨
        image_size: å›¾åƒå°ºå¯¸ (width, height)
        min_area_ratio: æœ€å°é¢ç§¯æ¯”ä¾‹ï¼ˆç›¸å¯¹äºå›¾åƒæ€»é¢ç§¯ï¼‰
        max_area_ratio: æœ€å¤§é¢ç§¯æ¯”ä¾‹ï¼ˆç›¸å¯¹äºå›¾åƒæ€»é¢ç§¯ï¼‰
    
    Returns:
        è¿‡æ»¤åçš„ (output_images, output_masks, detections_with_masks, object_names)
    """
    if not output_masks or detections_with_masks is None:
        return output_images, output_masks, detections_with_masks, object_names
    
    width, height = image_size
    total_area = width * height
    min_area_pixels = total_area * min_area_ratio
    max_area_pixels = total_area * max_area_ratio
    
    keep_indices = []
    filtered_reasons = []
    
    # è®¡ç®—æ¯ä¸ªmaskçš„é¢ç§¯å¹¶åˆ¤æ–­æ˜¯å¦ä¿ç•™
    for i, mask_tensor in enumerate(output_masks):
        # å°†tensorè½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è®¡ç®—é¢ç§¯
        if len(mask_tensor.shape) == 3:
            # å¦‚æœæ˜¯3D tensor (H, W, C)ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
            mask_array = mask_tensor[:, :, 0].numpy() > 0.5
        else:
            # å¦‚æœæ˜¯2D tensor (H, W)
            mask_array = mask_tensor.numpy() > 0.5
        
        mask_area = np.sum(mask_array)
        area_ratio = mask_area / total_area
        
        if mask_area < min_area_pixels:
            filtered_reasons.append(f"å¤ªå° (é¢ç§¯æ¯”ä¾‹: {area_ratio:.4f} < {min_area_ratio})")
        elif mask_area > max_area_pixels:
            filtered_reasons.append(f"å¤ªå¤§ (é¢ç§¯æ¯”ä¾‹: {area_ratio:.4f} > {max_area_ratio})")
        else:
            keep_indices.append(i)
    
    # å¦‚æœæœ‰è¢«è¿‡æ»¤çš„é¡¹ç›®ï¼Œæ‰“å°ä¿¡æ¯
    if len(keep_indices) < len(output_masks):
        filtered_count = len(output_masks) - len(keep_indices)
        print(f"VVL_GroundingDinoSAM2: åŸºäºé¢ç§¯è¿‡æ»¤æ‰ {filtered_count} ä¸ªåˆ†å‰²ç»“æœ:")
        for i, reason in enumerate(filtered_reasons):
            if i not in keep_indices:
                obj_name = object_names[i] if i < len(object_names) else f"object_{i+1}"
                print(f"  - {obj_name}: {reason}")
    
    # è¿‡æ»¤ç»“æœ
    if not keep_indices:
        # å¦‚æœæ‰€æœ‰ç»“æœéƒ½è¢«è¿‡æ»¤æ‰
        return [], [], None, []
    
    # è¿‡æ»¤output_imageså’Œoutput_masks
    filtered_output_images = [output_images[i] for i in keep_indices] if output_images else []
    filtered_output_masks = [output_masks[i] for i in keep_indices]
    filtered_object_names = [object_names[i] for i in keep_indices] if object_names else []
    
    # è¿‡æ»¤detections_with_masks
    if detections_with_masks is not None:
        # åˆ›å»ºæ–°çš„detectionså¯¹è±¡ï¼ŒåªåŒ…å«ä¿ç•™çš„ç´¢å¼•
        if hasattr(detections_with_masks, 'xyxy') and detections_with_masks.xyxy is not None:
            filtered_xyxy = detections_with_masks.xyxy[keep_indices]
        else:
            filtered_xyxy = None
            
        if hasattr(detections_with_masks, 'mask') and detections_with_masks.mask is not None:
            filtered_mask = detections_with_masks.mask[keep_indices]
        else:
            filtered_mask = None
            
        if hasattr(detections_with_masks, 'confidence') and detections_with_masks.confidence is not None:
            filtered_confidence = detections_with_masks.confidence[keep_indices]
        else:
            filtered_confidence = None
            
        if hasattr(detections_with_masks, 'class_id') and detections_with_masks.class_id is not None:
            filtered_class_id = detections_with_masks.class_id[keep_indices]
        else:
            filtered_class_id = None
        
        # åˆ›å»ºæ–°çš„Detectionså¯¹è±¡
        filtered_detections = sv.Detections(
            xyxy=filtered_xyxy,
            mask=filtered_mask,
            confidence=filtered_confidence,
            class_id=filtered_class_id
        )
        
        # å¤åˆ¶dataå­—å…¸
        if hasattr(detections_with_masks, 'data') and detections_with_masks.data:
            filtered_detections.data = {}
            for key, value in detections_with_masks.data.items():
                if isinstance(value, (list, np.ndarray)) and len(value) == len(output_masks):
                    if isinstance(value, list):
                        filtered_detections.data[key] = [value[i] for i in keep_indices]
                    else:
                        filtered_detections.data[key] = value[keep_indices]
                else:
                    filtered_detections.data[key] = value
    else:
        filtered_detections = None
    
    return filtered_output_images, filtered_output_masks, filtered_detections, filtered_object_names

def sam2_segment(sam_model, image, boxes):
    """SAM2 segmentation function adapted for SAM2"""
    if boxes.shape[0] == 0:
        return [], [], None
    
    # Convert PIL image to numpy
    image_np = np.array(image)
    image_np_rgb = image_np[..., :3]
    
    # Use SAM2 inference
    # Create a dummy sv.Detections object with the boxes
    detections = sv.Detections(xyxy=boxes.numpy())
    
    # Use the existing SAM2 inference function
    detections_with_masks = run_sam_inference(sam_model, image, detections)
    
    # Convert masks to the expected format
    output_masks = []
    output_images = []
    
    if detections_with_masks.mask is not None:
        for mask in detections_with_masks.mask:
            # Convert mask to PIL Image
            mask_pil = Image.fromarray((mask * 255).astype(np.uint8)).convert("L")
            output_masks.append(pil2tensor(mask_pil))
            
            # Create masked image
            image_np_copy = copy.deepcopy(image_np)
            if len(image_np_copy.shape) == 3:
                image_np_copy[~mask] = np.array([0, 0, 0])
            else:
                image_np_copy[~mask] = np.array([0, 0, 0, 0])
            
            masked_image_pil = Image.fromarray(image_np_copy)
            output_images.append(pil2tensor(masked_image_pil.convert("RGB")))
    
    return output_images, output_masks, detections_with_masks

# GroundingDINO model management functions (removed - now using dedicated loader node)

class VVL_GroundingDinoSAM2:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "sam2_model": ("SAM2MODEL", {"tooltip": "SAM2åˆ†å‰²æ¨¡å‹ï¼Œç”¨äºå¯¹æ£€æµ‹åˆ°çš„å¯¹è±¡è¿›è¡Œç²¾ç¡®åˆ†å‰²"}),
                "grounding_dino_model": ("GROUNDING_DINO_MODEL", {"tooltip": "GroundingDINOç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œç”¨äºæ ¹æ®æ–‡æœ¬æç¤ºæ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡"}),
                "image": ("IMAGE", {"tooltip": "è¾“å…¥çš„å›¾åƒï¼Œæ”¯æŒæ‰¹é‡å¤„ç†å¤šå¼ å›¾åƒ"}),
                "external_caption": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "tooltip": "å›¾åƒæè¿°æ–‡æœ¬æˆ–å¾…æ£€æµ‹å¯¹è±¡åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¦‚'person,car,dog'"
                }),
                "threshold": ("FLOAT", {
                    "default": 0.3, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.01,
                    "tooltip": "ç›®æ ‡æ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå€¼è¶Šé«˜æ£€æµ‹è¶Šä¸¥æ ¼ï¼Œå»ºè®®èŒƒå›´0.2-0.5ã€‚è¿‡ä½ä¼šäº§ç”Ÿè¯¯æ£€ï¼Œè¿‡é«˜å¯èƒ½æ¼æ£€"
                }),
                "iou_threshold": ("FLOAT", {
                    "default": 0.5, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.01,
                    "tooltip": "IoUé˜ˆå€¼ç”¨äºå»é™¤é‡å¤æ£€æµ‹æ¡†ï¼Œå€¼è¶Šé«˜ä¿ç•™çš„é‡å æ¡†è¶Šå¤šã€‚å»ºè®®0.3-0.7ï¼Œé¿å…åŒä¸€å¯¹è±¡è¢«é‡å¤åˆ†å‰²"
                }),
                "mask_containment_threshold": ("FLOAT", {
                    "default": 0.8, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.05,
                    "tooltip": "åŸºäºå®é™…maskå½¢çŠ¶çš„åŒ…å«é˜ˆå€¼ï¼Œç”¨äºç§»é™¤è¢«å…¶ä»–maskåŒ…å«çš„é‡å¤åˆ†å‰²ã€‚è®¾ä¸º0æ—¶ç¦ç”¨æ­¤åŠŸèƒ½ï¼Œå€¼è¶Šé«˜è¶Šä¸¥æ ¼ï¼Œ0.8è¡¨ç¤ºè¢«åŒ…å«80%ä»¥ä¸Šæ‰ç§»é™¤"
                }),
            },
            "optional": {
                "min_area_ratio": ("FLOAT", {
                    "default": 0.002, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.0001, 
                    "tooltip": "æœ€å°é¢ç§¯æ¯”ä¾‹ï¼ˆç›¸å¯¹äºå›¾åƒæ€»é¢ç§¯ï¼‰ï¼Œç”¨äºè¿‡æ»¤å¤ªå°çš„åˆ†å‰²ç»“æœã€‚0.002è¡¨ç¤ºå å›¾åƒ0.2%ä»¥ä¸‹çš„åŒºåŸŸå°†è¢«è¿‡æ»¤æ‰"
                }),
                "max_area_ratio": ("FLOAT", {
                    "default": 0.2, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.01, 
                    "tooltip": "æœ€å¤§é¢ç§¯æ¯”ä¾‹ï¼ˆç›¸å¯¹äºå›¾åƒæ€»é¢ç§¯ï¼‰ï¼Œç”¨äºè¿‡æ»¤å¤ªå¤§çš„åˆ†å‰²ç»“æœã€‚0.2è¡¨ç¤ºå å›¾åƒ20%ä»¥ä¸Šçš„åŒºåŸŸå°†è¢«è¿‡æ»¤æ‰ï¼Œé¿å…èƒŒæ™¯è¯¯æ£€"
                }),
                "remaining_area_mask": ("MASK", {
                    "tooltip": "å¯é€‰ï¼šç”¨äºè®¡ç®—å‰©ä½™åŒºåŸŸçš„è¾“å…¥maskã€‚å½“æä¾›æ—¶ï¼ŒèŠ‚ç‚¹å°†åœ¨è¾“å‡ºobject_masksä¸­æ–°å¢ä¸€ä¸ªmaskï¼Œä»£è¡¨(è¾“å…¥mask âˆ© éå·²åˆ†å‰²åŒºåŸŸ)"
                }),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", "STRING", "STRING",)
    RETURN_NAMES = ("annotated_image", "object_masks", "detection_json", "object_names",)
    FUNCTION = "_process_image"
    CATEGORY = "ğŸ’ƒrDancer"
    OUTPUT_IS_LIST = (False, True, False, True)

    def _process_image(self, sam2_model, grounding_dino_model, image, external_caption, 
                      threshold=0.3, iou_threshold=0.5, mask_containment_threshold=0.8,
                      min_area_ratio=0.002, max_area_ratio=0.2, remaining_area_mask=None):
        
        try:
            # è¾“å…¥éªŒè¯å’Œå®‰å…¨æ£€æŸ¥
            if not isinstance(image, torch.Tensor):
                raise ValueError(f"Expected image to be torch.Tensor, got {type(image)}")
            
            if image.dim() != 4:
                raise ValueError(f"Expected image to have 4 dimensions (batch, height, width, channels), got {image.dim()}")
                
            batch_size = image.shape[0]
            print(f"VVL_GroundingDinoSAM2: Processing {batch_size} images")
            
            # ä»SAM2æ¨¡å‹å­—å…¸ä¸­è·å–æ¨¡å‹å’Œè®¾å¤‡ä¿¡æ¯
            if not isinstance(sam2_model, dict) or 'model' not in sam2_model:
                raise ValueError("Invalid sam2_model format")
                
            sam2_model_instance = sam2_model['model']
            device = sam2_model.get('device', 'cpu')
            
            # ä»ä¼ å…¥çš„grounding_dino_modelå­—å…¸ä¸­è·å–æ¨¡å‹
            if not isinstance(grounding_dino_model, dict) or 'model' not in grounding_dino_model:
                raise ValueError("Invalid grounding_dino_model format")
                
            grounding_dino_model_instance = grounding_dino_model['model']
            
            # æ¸…ç†è¾“å…¥æ–‡æœ¬
            external_caption_clean = external_caption.strip() if external_caption else ""
            
            # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
            annotated_images = []
            object_masks_list = []
            detection_jsons = []
            final_object_names = []
            
            # å¤„ç†æ¯å¼ å›¾åƒ
            for i in range(batch_size):
                try:
                    img_tensor = image[i]
                    img_pil = tensor2pil(img_tensor).convert("RGB")
                    
                    print(f"VVL_GroundingDinoSAM2: Processing image {i+1}/{batch_size}")
                    
                    # è§£ææ£€æµ‹çŸ­è¯­
                    current_detection_phrases = []
                    if external_caption_clean:
                        current_detection_phrases = [c.strip() for c in external_caption_clean.split(',') if c.strip()]
                        if not current_detection_phrases and external_caption_clean:
                            current_detection_phrases = [external_caption_clean.strip()]
                        print(f"VVL_GroundingDinoSAM2: Image {i+1} - Detection phrases: {current_detection_phrases}")
                    else:
                        print(f"VVL_GroundingDinoSAM2: Image {i+1} - No external_caption provided")
                    
                    # æ‰§è¡Œç›®æ ‡æ£€æµ‹
                    object_names = []
                    all_boxes_list = []
                    
                    if not current_detection_phrases:
                        boxes = torch.zeros((0, 4))
                    else:
                        for phrase in current_detection_phrases:
                            boxes_single = groundingdino_predict(grounding_dino_model_instance, img_pil, phrase, threshold)
                            if boxes_single.shape[0] > 0:
                                all_boxes_list.append(boxes_single)
                                object_names.extend([phrase] * boxes_single.shape[0])
                        
                        boxes = torch.cat(all_boxes_list, dim=0) if all_boxes_list else torch.zeros((0, 4))
                    
                    # åå¤‡é€»è¾‘ï¼šå¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°å¯¹è±¡ï¼Œå°è¯•é™ä½é˜ˆå€¼
                    if boxes.shape[0] == 0 and threshold > 0.15 and current_detection_phrases:
                        fallback_thresh = max(0.1, threshold * 0.5)
                        print(f"VVL_GroundingDinoSAM2: Image {i+1} - Retrying with lower threshold {fallback_thresh}")
                        
                        for phrase in current_detection_phrases:
                            boxes_single = groundingdino_predict(grounding_dino_model_instance, img_pil, phrase, fallback_thresh)
                            if boxes_single.shape[0] > 0:
                                all_boxes_list.append(boxes_single)
                                object_names.extend([phrase] * boxes_single.shape[0])
                        
                        boxes = torch.cat(all_boxes_list, dim=0) if all_boxes_list else torch.zeros((0, 4))
                    
                    print(f"VVL_GroundingDinoSAM2: Image {i+1} - Found {boxes.shape[0]} boxes")
                    
                    if boxes.shape[0] == 0:
                        # æ²¡æœ‰æ£€æµ‹åˆ°å¯¹è±¡ï¼Œè¿”å›åŸå§‹å›¾åƒ
                        annotated_images.append(pil2tensor(img_pil))
                        detection_jsons.append(json.dumps({
                            "image_width": img_pil.width,
                            "image_height": img_pil.height,
                            "objects": []
                        }, ensure_ascii=False, indent=2))
                        continue
                    
                    # å»é‡å¤è¾¹ç•Œæ¡†
                    if boxes.shape[0] > 0:
                        boxes, object_names = remove_duplicate_boxes(boxes, object_names, iou_threshold)
                        print(f"VVL_GroundingDinoSAM2: Image {i+1} - After deduplication: {boxes.shape[0]} boxes")
                    
                    # SAM2åˆ†å‰²
                    output_images, output_masks, detections_with_masks = sam2_segment(sam2_model_instance, img_pil, boxes)
                    
                    # åº”ç”¨å„ç§è¿‡æ»¤å™¨
                    if output_masks and len(output_masks) > 1 and mask_containment_threshold > 0:
                        output_images, output_masks, detections_with_masks, object_names = remove_duplicate_masks_by_containment(
                            output_images, output_masks, detections_with_masks, object_names, containment_threshold=mask_containment_threshold
                        )
                    
                    if output_masks and (min_area_ratio > 0 or max_area_ratio < 1.0):
                        output_images, output_masks, detections_with_masks, object_names = filter_by_area(
                            output_images, output_masks, detections_with_masks, object_names, 
                            (img_pil.width, img_pil.height), min_area_ratio, max_area_ratio
                        )
                    
                    # å¤„ç†å‰©ä½™åŒºåŸŸmask
                    if remaining_area_mask is not None and output_masks:
                        try:
                            if remaining_area_mask.dim() == 4 and i < remaining_area_mask.shape[0]:
                                cur_input_mask = remaining_area_mask[i]
                            elif remaining_area_mask.dim() == 3:
                                cur_input_mask = remaining_area_mask
                            else:
                                print(f"VVL_GroundingDinoSAM2: Warning - Cannot process remaining_area_mask for image {i+1}")
                                cur_input_mask = None
                            
                            if cur_input_mask is not None:
                                # å¤„ç†å‰©ä½™åŒºåŸŸé€»è¾‘...
                                input_mask_np = cur_input_mask.cpu().numpy()
                                if input_mask_np.ndim == 3:
                                    if input_mask_np.shape[0] == 1:
                                        input_mask_np = input_mask_np[0]
                                    else:
                                        input_mask_np = input_mask_np[:, :, 0]
                                input_mask_bool = np.squeeze(input_mask_np) > 0.5
                                
                                # åˆå¹¶å·²æœ‰mask
                                combined_existing = np.zeros_like(input_mask_bool, dtype=bool)
                                for m_tensor in output_masks:
                                    m_np = m_tensor.cpu().numpy()
                                    if m_np.ndim == 3:
                                        if m_np.shape[0] == 1:
                                            m_np = m_np[0]
                                        else:
                                            m_np = m_np[:, :, 0]
                                    combined_existing |= (m_np > 0.5)
                                
                                # è®¡ç®—å‰©ä½™åŒºåŸŸ
                                remain_bool = np.logical_and(input_mask_bool, np.logical_not(combined_existing))
                                
                                if np.sum(remain_bool) > 0:
                                    remain_uint8 = (remain_bool * 255).astype(np.uint8)
                                    remain_cleaned = remove_small_regions(remain_uint8, keep_largest_n=1)
                                    remain_bool_cleaned = remain_cleaned > 127
                                    
                                    if np.sum(remain_bool_cleaned) > 0:
                                        remain_mask_pil = Image.fromarray(remain_cleaned).convert("L")
                                        remain_mask_tensor = pil2tensor(remain_mask_pil)
                                        output_masks.append(remain_mask_tensor)
                                        
                                        img_np_copy = copy.deepcopy(np.array(img_pil))
                                        img_np_copy[~remain_bool_cleaned] = np.array([0, 0, 0])
                                        remain_image_pil = Image.fromarray(img_np_copy)
                                        output_images.append(pil2tensor(remain_image_pil.convert("RGB")))
                                        
                                        # æ›´æ–°æ£€æµ‹ç»“æœ
                                        ys, xs = np.where(remain_bool_cleaned)
                                        x_min, x_max = int(xs.min()), int(xs.max())
                                        y_min, y_max = int(ys.min()), int(ys.max())
                                        bbox_tensor = torch.tensor([x_min, y_min, x_max, y_max], dtype=torch.float32)
                                        
                                        if detections_with_masks is None:
                                            detections_with_masks = sv.Detections(xyxy=bbox_tensor.unsqueeze(0), mask=np.asarray([remain_bool_cleaned]))
                                        else:
                                            if hasattr(detections_with_masks, 'xyxy') and detections_with_masks.xyxy is not None:
                                                if isinstance(detections_with_masks.xyxy, np.ndarray):
                                                    bbox_np = bbox_tensor.cpu().numpy()[None, :]
                                                    detections_with_masks.xyxy = np.concatenate([detections_with_masks.xyxy, bbox_np], axis=0)
                                                else:
                                                    detections_with_masks.xyxy = torch.cat([detections_with_masks.xyxy, bbox_tensor.unsqueeze(0)], dim=0)
                                            
                                            if hasattr(detections_with_masks, 'mask') and detections_with_masks.mask is not None:
                                                detections_with_masks.mask = np.concatenate([detections_with_masks.mask, remain_bool_cleaned[None, :, :]], axis=0)
                                        
                                        object_names.append("remaining_area")
                        
                        except Exception as e:
                            print(f"VVL_GroundingDinoSAM2: Error processing remaining_area_mask for image {i+1}: {e}")
                    
                    # è§£å†³é‡å¤åç§°
                    if object_names:
                        object_names = resolve_duplicate_names(object_names)
                    
                    # éªŒè¯æ•°æ®ä¸€è‡´æ€§
                    verify_data_consistency(object_names, detections_with_masks, output_masks, image_index=i+1)
                    
                    # æ·»åŠ åˆ°æœ€ç»ˆç»“æœ
                    final_object_names.extend(object_names)
                    
                    # æ ‡æ³¨å›¾åƒ
                    if len(object_names) > 0 and detections_with_masks is not None:
                        labels = []
                        detection_count = len(detections_with_masks)
                        
                        for j in range(detection_count):
                            if j < len(object_names):
                                labels.append(object_names[j])
                            else:
                                labels.append(f"object_{j+1}")
                        
                        if not hasattr(detections_with_masks, 'data') or detections_with_masks.data is None:
                            detections_with_masks.data = {}
                        detections_with_masks.data['class_name'] = labels
                        
                        annotated_img = annotate_image(img_pil, detections_with_masks)
                        annotated_images.append(pil2tensor(annotated_img))
                    else:
                        annotated_images.append(pil2tensor(img_pil))
                    
                    # æ·»åŠ masksåˆ°åˆ—è¡¨
                    if output_masks:
                        object_masks_list.extend(output_masks)
                    
                    # åˆ›å»ºæ£€æµ‹JSON
                    detection_json = {
                        "image_width": img_pil.width,
                        "image_height": img_pil.height,
                        "objects": []
                    }
                    
                    if detections_with_masks is not None and hasattr(detections_with_masks, 'xyxy') and detections_with_masks.xyxy is not None:
                        for j, bbox in enumerate(detections_with_masks.xyxy):
                            bbox_2d = [int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])]
                            obj_name = object_names[j] if j < len(object_names) else f"object_{j+1}"
                            detection_json["objects"].append({
                                "name": obj_name,
                                "bbox_2d": bbox_2d
                            })
                    
                    json_str = json.dumps(detection_json, ensure_ascii=False, indent=2)
                    json_str = re.sub(r'"bbox_2d":\s*\[\s*(\d+),\s*(\d+),\s*(\d+),\s*(\d+)\s*\]', 
                                     r'"bbox_2d": [\1,\2,\3,\4]', json_str)
                    detection_jsons.append(json_str)
                    
                except Exception as e:
                    print(f"VVL_GroundingDinoSAM2: Error processing image {i+1}: {e}")
                    # æ·»åŠ ç©ºç»“æœä»¥ä¿æŒæ‰¹æ¬¡ä¸€è‡´æ€§
                    annotated_images.append(pil2tensor(tensor2pil(img_tensor).convert("RGB")))
                    detection_jsons.append(json.dumps({
                        "image_width": 512,
                        "image_height": 512,
                        "objects": []
                    }, ensure_ascii=False, indent=2))
            
            # å †å ç»“æœ
            annotated_images_stacked = torch.stack(annotated_images) if annotated_images else torch.empty(0)
            final_detection_json = detection_jsons[0] if detection_jsons else "{}"
            
            print(f"VVL_GroundingDinoSAM2: Processing completed. Generated {len(object_masks_list)} masks")
            
            return (annotated_images_stacked, object_masks_list, final_detection_json, final_object_names)
            
        except Exception as e:
            print(f"VVL_GroundingDinoSAM2: Critical error: {e}")
            import traceback
            traceback.print_exc()
            
            # è¿”å›å®‰å…¨çš„é»˜è®¤ç»“æœ
            batch_size = image.shape[0] if hasattr(image, 'shape') else 1
            empty_images = torch.zeros((batch_size, 512, 512, 3))
            empty_json = json.dumps({"image_width": 512, "image_height": 512, "objects": []})
            
            return (empty_images, [], empty_json, [])

# Node registration
NODE_CLASS_MAPPINGS = {
    "VVL_GroundingDinoSAM2": VVL_GroundingDinoSAM2
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VVL_GroundingDinoSAM2": "VVL GroundingDINO + SAM2"
}

__all__ = ["NODE_CLASS_MAPPINGS", "NODE_DISPLAY_NAME_MAPPINGS"] 