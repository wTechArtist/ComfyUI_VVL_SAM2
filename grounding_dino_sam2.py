import torch
from PIL import Image, ImageDraw
import numpy as np
import os
import json
import re
import copy
import supervision as sv
from typing import Tuple, Optional, Any, List

try:
    from florence_sam_processor import process_image
    from utils.sam import model_to_config_map as sam_model_to_config_map
    from utils.sam import load_sam_image_model, run_sam_inference
    from utils.florence import load_florence_model, run_florence_inference, \
        FLORENCE_OPEN_VOCABULARY_DETECTION_TASK, FLORENCE_DETAILED_CAPTION_TASK, FLORENCE_CAPTION_TO_PHRASE_GROUNDING_TASK
    from utils.modes import IMAGE_INFERENCE_MODES, IMAGE_OPEN_VOCABULARY_DETECTION_MODE, IMAGE_CAPTION_GROUNDING_MASKS_MODE, VIDEO_INFERENCE_MODES
    from mask_cleaner import remove_small_regions
except ImportError:
    # We're running as a module
    from .florence_sam_processor import process_image
    from .utils.sam import model_to_config_map as sam_model_to_config_map
    from .utils.sam import load_sam_image_model, run_sam_inference
    from .utils.florence import load_florence_model, run_florence_inference, \
        FLORENCE_OPEN_VOCABULARY_DETECTION_TASK, FLORENCE_DETAILED_CAPTION_TASK, FLORENCE_CAPTION_TO_PHRASE_GROUNDING_TASK
    from .utils.modes import IMAGE_INFERENCE_MODES, IMAGE_OPEN_VOCABULARY_DETECTION_MODE, IMAGE_CAPTION_GROUNDING_MASKS_MODE, VIDEO_INFERENCE_MODES
    from .mask_cleaner import remove_small_regions

# GroundingDINO imports (adapted from node.py)
import logging
from torch.hub import download_url_to_file
from urllib.parse import urlparse
import folder_paths
import comfy.model_management
import glob

# GroundingDINO specific imports
try:
    from local_groundingdino.datasets import transforms as T
    from local_groundingdino.util.utils import clean_state_dict as local_groundingdino_clean_state_dict
    from local_groundingdino.util.slconfig import SLConfig as local_groundingdino_SLConfig
    from local_groundingdino.models import build_model as local_groundingdino_build_model
except ImportError:
    print("Warning: GroundingDINO dependencies not found. Please install them.")
    T = None
    local_groundingdino_clean_state_dict = None
    local_groundingdino_SLConfig = None
    local_groundingdino_build_model = None

logger = logging.getLogger('vvl_GroundingDinoSAM2')

# GroundingDINO model configurations
groundingdino_model_dir_name = "grounding-dino"
groundingdino_model_list = {
    "GroundingDINO_SwinT_OGC (694MB)": {
        "config_url": "https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/GroundingDINO_SwinT_OGC.cfg.py",
        "model_url": "https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/groundingdino_swint_ogc.pth",
    },
    "GroundingDINO_SwinB (938MB)": {
        "config_url": "https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/GroundingDINO_SwinB.cfg.py",
        "model_url": "https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/groundingdino_swinb_cogcoor.pth"
    },
}

# Format conversion helpers
def tensor2pil(t_image: torch.Tensor) -> Image.Image:
    return Image.fromarray(np.clip(255.0 * t_image.cpu().numpy(), 0, 255).astype(np.uint8))

def pil2tensor(image: Image.Image) -> torch.Tensor:
    return torch.from_numpy(np.array(image).astype(np.float32) / 255.0)

# GroundingDINO utility functions (adapted from node.py)
def get_bert_base_uncased_model_path():
    comfy_bert_model_base = os.path.join(folder_paths.models_dir, 'bert-base-uncased')
    if glob.glob(os.path.join(comfy_bert_model_base, '**/model.safetensors'), recursive=True):
        print('grounding-dino is using models/bert-base-uncased')
        return comfy_bert_model_base
    return 'bert-base-uncased'

def get_local_filepath(url, dirname, local_file_name=None):
    if not local_file_name:
        parsed_url = urlparse(url)
        local_file_name = os.path.basename(parsed_url.path)

    destination = folder_paths.get_full_path(dirname, local_file_name)
    if destination:
        logger.warn(f'using extra model: {destination}')
        return destination

    folder = os.path.join(folder_paths.models_dir, dirname)
    if not os.path.exists(folder):
        os.makedirs(folder)

    destination = os.path.join(folder, local_file_name)
    if not os.path.exists(destination):
        logger.warn(f'downloading {url} to {destination}')
        download_url_to_file(url, destination)
    return destination

def load_groundingdino_model(model_name):
    if local_groundingdino_SLConfig is None:
        raise ImportError("GroundingDINO dependencies not available")
        
    dino_model_args = local_groundingdino_SLConfig.fromfile(
        get_local_filepath(
            groundingdino_model_list[model_name]["config_url"],
            groundingdino_model_dir_name
        ),
    )

    if dino_model_args.text_encoder_type == 'bert-base-uncased':
        dino_model_args.text_encoder_type = get_bert_base_uncased_model_path()
    
    dino = local_groundingdino_build_model(dino_model_args)
    checkpoint = torch.load(
        get_local_filepath(
            groundingdino_model_list[model_name]["model_url"],
            groundingdino_model_dir_name,
        ),
    )
    dino.load_state_dict(local_groundingdino_clean_state_dict(
        checkpoint['model']), strict=False)
    device = comfy.model_management.get_torch_device()
    dino.to(device=device)
    dino.eval()
    return dino

def groundingdino_predict(dino_model, image, prompt, threshold):
    def load_dino_image(image_pil):
        transform = T.Compose([
            T.RandomResize([800], max_size=1333),
            T.ToTensor(),
            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])
        image, _ = transform(image_pil, None)  # 3, h, w
        return image

    def get_grounding_output(model, image, caption, box_threshold):
        caption = caption.lower()
        caption = caption.strip()
        if not caption.endswith("."):
            caption = caption + "."
        device = comfy.model_management.get_torch_device()
        image = image.to(device)
        with torch.no_grad():
            outputs = model(image[None], captions=[caption])
        logits = outputs["pred_logits"].sigmoid()[0]  # (nq, 256)
        boxes = outputs["pred_boxes"][0]  # (nq, 4)
        # filter output
        logits_filt = logits.clone()
        boxes_filt = boxes.clone()
        filt_mask = logits_filt.max(dim=1)[0] > box_threshold
        logits_filt = logits_filt[filt_mask]  # num_filt, 256
        boxes_filt = boxes_filt[filt_mask]  # num_filt, 4
        return boxes_filt.cpu()

    dino_image = load_dino_image(image.convert("RGB"))
    boxes_filt = get_grounding_output(dino_model, dino_image, prompt, threshold)
    H, W = image.size[1], image.size[0]
    for i in range(boxes_filt.size(0)):
        boxes_filt[i] = boxes_filt[i] * torch.Tensor([W, H, W, H])
        boxes_filt[i][:2] -= boxes_filt[i][2:] / 2
        boxes_filt[i][2:] += boxes_filt[i][:2]
    return boxes_filt

# Color palette and annotators for image annotation
COLORS = ['#FF1493', '#00BFFF', '#FF6347', '#FFD700', '#32CD32', '#8A2BE2']
COLOR_PALETTE = sv.ColorPalette.from_hex(COLORS)
BOX_ANNOTATOR = sv.BoxAnnotator(color=COLOR_PALETTE, color_lookup=sv.ColorLookup.INDEX)
LABEL_ANNOTATOR = sv.LabelAnnotator(
    color=COLOR_PALETTE,
    color_lookup=sv.ColorLookup.INDEX,
    text_position=sv.Position.CENTER_OF_MASS,
    text_color=sv.Color.from_hex("#000000"),
    border_radius=5
)
MASK_ANNOTATOR = sv.MaskAnnotator(
    color=COLOR_PALETTE,
    color_lookup=sv.ColorLookup.INDEX
)

def annotate_image(image, detections):
    output_image = image.copy()
    output_image = MASK_ANNOTATOR.annotate(output_image, detections)
    output_image = BOX_ANNOTATOR.annotate(output_image, detections)
    output_image = LABEL_ANNOTATOR.annotate(output_image, detections)
    return output_image

def calculate_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoU (Intersection over Union)"""
    # boxæ ¼å¼: [x1, y1, x2, y2]
    x1_inter = max(box1[0], box2[0])
    y1_inter = max(box1[1], box2[1])
    x2_inter = min(box1[2], box2[2])
    y2_inter = min(box1[3], box2[3])
    
    # è®¡ç®—äº¤é›†é¢ç§¯
    if x2_inter <= x1_inter or y2_inter <= y1_inter:
        return 0.0
    
    inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
    
    # è®¡ç®—å¹¶é›†é¢ç§¯
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union_area = box1_area + box2_area - inter_area
    
    if union_area <= 0:
        return 0.0
    
    return inter_area / union_area

def remove_duplicate_boxes(boxes, object_names, iou_threshold=0.5):
    """ä½¿ç”¨NMSç®—æ³•å»é™¤é‡å¤çš„è¾¹ç•Œæ¡†"""
    if boxes.shape[0] == 0:
        return boxes, object_names
    
    # è®¡ç®—æ¯ä¸ªæ¡†çš„é¢ç§¯
    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
    
    # æŒ‰é¢ç§¯æ’åºï¼ˆä¿ç•™è¾ƒå¤§çš„æ¡†ï¼‰
    order = torch.argsort(areas, descending=True)
    
    keep_indices = []
    remaining = order.tolist()
    
    while remaining:
        # å–å‡ºå½“å‰æœ€å¤§é¢ç§¯çš„æ¡†
        current_idx = remaining[0]
        keep_indices.append(current_idx)
        remaining.remove(current_idx)
        
        if not remaining:
            break
        
        # è®¡ç®—å½“å‰æ¡†ä¸å…¶ä½™æ¡†çš„IoU
        current_box = boxes[current_idx]
        to_remove = []
        
        for other_idx in remaining:
            other_box = boxes[other_idx]
            iou = calculate_iou(current_box, other_box)
            
            if iou > iou_threshold:
                to_remove.append(other_idx)
        
        # ç§»é™¤é‡å çš„æ¡†
        for idx in to_remove:
            remaining.remove(idx)
    
    # è¿”å›å»é‡åçš„boxeså’Œå¯¹åº”çš„object_names
    keep_indices = sorted(keep_indices)
    filtered_boxes = boxes[keep_indices]
    filtered_object_names = [object_names[i] for i in keep_indices] if object_names else []
    
    return filtered_boxes, filtered_object_names

def filter_by_area(output_images, output_masks, detections_with_masks, object_names, image_size, 
                  min_area_ratio=0.0001, max_area_ratio=0.9):
    """
    æ ¹æ®maské¢ç§¯å¤§å°è¿‡æ»¤åˆ†å‰²ç»“æœ
    
    Args:
        output_images: SAM2åˆ†å‰²å¾—åˆ°çš„é®ç½©å›¾åƒåˆ—è¡¨
        output_masks: SAM2åˆ†å‰²å¾—åˆ°çš„maskåˆ—è¡¨  
        detections_with_masks: supervision.Detectionså¯¹è±¡
        object_names: å¯¹è±¡åç§°åˆ—è¡¨
        image_size: å›¾åƒå°ºå¯¸ (width, height)
        min_area_ratio: æœ€å°é¢ç§¯æ¯”ä¾‹ï¼ˆç›¸å¯¹äºå›¾åƒæ€»é¢ç§¯ï¼‰
        max_area_ratio: æœ€å¤§é¢ç§¯æ¯”ä¾‹ï¼ˆç›¸å¯¹äºå›¾åƒæ€»é¢ç§¯ï¼‰
    
    Returns:
        è¿‡æ»¤åçš„ (output_images, output_masks, detections_with_masks, object_names)
    """
    if not output_masks or detections_with_masks is None:
        return output_images, output_masks, detections_with_masks, object_names
    
    width, height = image_size
    total_area = width * height
    min_area_pixels = total_area * min_area_ratio
    max_area_pixels = total_area * max_area_ratio
    
    keep_indices = []
    filtered_reasons = []
    
    # è®¡ç®—æ¯ä¸ªmaskçš„é¢ç§¯å¹¶åˆ¤æ–­æ˜¯å¦ä¿ç•™
    for i, mask_tensor in enumerate(output_masks):
        # å°†tensorè½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è®¡ç®—é¢ç§¯
        if len(mask_tensor.shape) == 3:
            # å¦‚æœæ˜¯3D tensor (H, W, C)ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
            mask_array = mask_tensor[:, :, 0].numpy() > 0.5
        else:
            # å¦‚æœæ˜¯2D tensor (H, W)
            mask_array = mask_tensor.numpy() > 0.5
        
        mask_area = np.sum(mask_array)
        area_ratio = mask_area / total_area
        
        if mask_area < min_area_pixels:
            filtered_reasons.append(f"å¤ªå° (é¢ç§¯æ¯”ä¾‹: {area_ratio:.4f} < {min_area_ratio})")
        elif mask_area > max_area_pixels:
            filtered_reasons.append(f"å¤ªå¤§ (é¢ç§¯æ¯”ä¾‹: {area_ratio:.4f} > {max_area_ratio})")
        else:
            keep_indices.append(i)
    
    # å¦‚æœæœ‰è¢«è¿‡æ»¤çš„é¡¹ç›®ï¼Œæ‰“å°ä¿¡æ¯
    if len(keep_indices) < len(output_masks):
        filtered_count = len(output_masks) - len(keep_indices)
        print(f"VVL_GroundingDinoSAM2: åŸºäºé¢ç§¯è¿‡æ»¤æ‰ {filtered_count} ä¸ªåˆ†å‰²ç»“æœ:")
        for i, reason in enumerate(filtered_reasons):
            if i not in keep_indices:
                obj_name = object_names[i] if i < len(object_names) else f"object_{i+1}"
                print(f"  - {obj_name}: {reason}")
    
    # è¿‡æ»¤ç»“æœ
    if not keep_indices:
        # å¦‚æœæ‰€æœ‰ç»“æœéƒ½è¢«è¿‡æ»¤æ‰
        return [], [], None, []
    
    # è¿‡æ»¤output_imageså’Œoutput_masks
    filtered_output_images = [output_images[i] for i in keep_indices] if output_images else []
    filtered_output_masks = [output_masks[i] for i in keep_indices]
    filtered_object_names = [object_names[i] for i in keep_indices] if object_names else []
    
    # è¿‡æ»¤detections_with_masks
    if detections_with_masks is not None:
        # åˆ›å»ºæ–°çš„detectionså¯¹è±¡ï¼ŒåªåŒ…å«ä¿ç•™çš„ç´¢å¼•
        if hasattr(detections_with_masks, 'xyxy') and detections_with_masks.xyxy is not None:
            filtered_xyxy = detections_with_masks.xyxy[keep_indices]
        else:
            filtered_xyxy = None
            
        if hasattr(detections_with_masks, 'mask') and detections_with_masks.mask is not None:
            filtered_mask = detections_with_masks.mask[keep_indices]
        else:
            filtered_mask = None
            
        if hasattr(detections_with_masks, 'confidence') and detections_with_masks.confidence is not None:
            filtered_confidence = detections_with_masks.confidence[keep_indices]
        else:
            filtered_confidence = None
            
        if hasattr(detections_with_masks, 'class_id') and detections_with_masks.class_id is not None:
            filtered_class_id = detections_with_masks.class_id[keep_indices]
        else:
            filtered_class_id = None
        
        # åˆ›å»ºæ–°çš„Detectionså¯¹è±¡
        filtered_detections = sv.Detections(
            xyxy=filtered_xyxy,
            mask=filtered_mask,
            confidence=filtered_confidence,
            class_id=filtered_class_id
        )
        
        # å¤åˆ¶dataå­—å…¸
        if hasattr(detections_with_masks, 'data') and detections_with_masks.data:
            filtered_detections.data = {}
            for key, value in detections_with_masks.data.items():
                if isinstance(value, (list, np.ndarray)) and len(value) == len(output_masks):
                    if isinstance(value, list):
                        filtered_detections.data[key] = [value[i] for i in keep_indices]
                    else:
                        filtered_detections.data[key] = value[keep_indices]
                else:
                    filtered_detections.data[key] = value
    else:
        filtered_detections = None
    
    return filtered_output_images, filtered_output_masks, filtered_detections, filtered_object_names

def sam2_segment(sam_model, image, boxes):
    """SAM2 segmentation function adapted for SAM2"""
    if boxes.shape[0] == 0:
        return [], [], None
    
    # Convert PIL image to numpy
    image_np = np.array(image)
    image_np_rgb = image_np[..., :3]
    
    # Use SAM2 inference
    # Create a dummy sv.Detections object with the boxes
    detections = sv.Detections(xyxy=boxes.numpy())
    
    # Use the existing SAM2 inference function
    detections_with_masks = run_sam_inference(sam_model, image, detections)
    
    # Convert masks to the expected format
    output_masks = []
    output_images = []
    
    if detections_with_masks.mask is not None:
        for mask in detections_with_masks.mask:
            # Convert mask to PIL Image
            mask_pil = Image.fromarray((mask * 255).astype(np.uint8)).convert("L")
            output_masks.append(pil2tensor(mask_pil))
            
            # Create masked image
            image_np_copy = copy.deepcopy(image_np)
            if len(image_np_copy.shape) == 3:
                image_np_copy[~mask] = np.array([0, 0, 0])
            else:
                image_np_copy[~mask] = np.array([0, 0, 0, 0])
            
            masked_image_pil = Image.fromarray(image_np_copy)
            output_images.append(pil2tensor(masked_image_pil.convert("RGB")))
    
    return output_images, output_masks, detections_with_masks

# Global variables for GroundingDINO and Florence2 model management
GROUNDING_DINO_MODEL = None
FLORENCE_MODEL = None
FLORENCE_PROCESSOR = None
CURRENT_GROUNDING_DINO_MODEL_NAME = None

def lazy_load_grounding_dino_florence_models(grounding_dino_model_name: str, load_florence2: bool = True):
    global GROUNDING_DINO_MODEL, FLORENCE_MODEL, FLORENCE_PROCESSOR, CURRENT_GROUNDING_DINO_MODEL_NAME
    
    # Load GroundingDINO model
    if GROUNDING_DINO_MODEL is None or CURRENT_GROUNDING_DINO_MODEL_NAME != grounding_dino_model_name:
        GROUNDING_DINO_MODEL = load_groundingdino_model(grounding_dino_model_name)
        CURRENT_GROUNDING_DINO_MODEL_NAME = grounding_dino_model_name
    
    # Load Florence-2 model (for caption generation when needed)
    if load_florence2 and (FLORENCE_MODEL is None or FLORENCE_PROCESSOR is None):
        device = comfy.model_management.get_torch_device()
        try:
            from utils.florence import load_florence_model
        except ImportError:
            from .utils.florence import load_florence_model
        FLORENCE_MODEL, FLORENCE_PROCESSOR = load_florence_model(device=device)

class VVL_GroundingDinoSAM2:
    @classmethod
    def INPUT_TYPES(cls):
        grounding_dino_models = list(groundingdino_model_list.keys())
        
        return {
            "required": {
                "sam2_model": ("VVL_SAM2_MODEL", {"tooltip": "SAM2åˆ†å‰²æ¨¡å‹ï¼Œç”¨äºå¯¹æ£€æµ‹åˆ°çš„å¯¹è±¡è¿›è¡Œç²¾ç¡®åˆ†å‰²"}),
                "grounding_dino_model": (grounding_dino_models, {
                    "default": grounding_dino_models[0],
                    "tooltip": "GroundingDINOç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œç”¨äºæ ¹æ®æ–‡æœ¬æç¤ºæ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡ã€‚SwinT_OGCæ¨¡å‹è¾ƒå°ä½†é€Ÿåº¦å¿«ï¼ŒSwinBæ¨¡å‹è¾ƒå¤§ä½†ç²¾åº¦æ›´é«˜"
                }),
                "image": ("IMAGE", {"tooltip": "è¾“å…¥çš„å›¾åƒï¼Œæ”¯æŒæ‰¹é‡å¤„ç†å¤šå¼ å›¾åƒ"}),
                "prompt": ("STRING", {
                    "default": "",
                    "tooltip": "ç›®æ ‡æ£€æµ‹çš„æ–‡æœ¬æç¤ºè¯ï¼Œç”¨é€—å·åˆ†éš”å¤šä¸ªå¯¹è±¡ï¼Œå¦‚'person,car,dog'ã€‚ç•™ç©ºæ—¶å°†ä½¿ç”¨Florence-2è‡ªåŠ¨ç”Ÿæˆæè¿°"
                }),
                "threshold": ("FLOAT", {
                    "default": 0.3, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.01,
                    "tooltip": "ç›®æ ‡æ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå€¼è¶Šé«˜æ£€æµ‹è¶Šä¸¥æ ¼ï¼Œå»ºè®®èŒƒå›´0.2-0.5ã€‚è¿‡ä½ä¼šäº§ç”Ÿè¯¯æ£€ï¼Œè¿‡é«˜å¯èƒ½æ¼æ£€"
                }),
                "iou_threshold": ("FLOAT", {
                    "default": 0.5, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.01,
                    "tooltip": "IoUé˜ˆå€¼ç”¨äºå»é™¤é‡å¤æ£€æµ‹æ¡†ï¼Œå€¼è¶Šé«˜ä¿ç•™çš„é‡å æ¡†è¶Šå¤šã€‚å»ºè®®0.3-0.7ï¼Œé¿å…åŒä¸€å¯¹è±¡è¢«é‡å¤åˆ†å‰²"
                }),
            },
            "optional": {
                "external_caption": ("STRING", {
                    "multiline": True, 
                    "default": "",
                    "tooltip": "å¤–éƒ¨æä¾›çš„å›¾åƒæè¿°æ–‡æœ¬ï¼Œç”¨é€—å·åˆ†éš”å¤šä¸ªå¯¹è±¡ã€‚å½“promptä¸ºç©ºæ—¶ï¼Œä¼˜å…ˆä½¿ç”¨æ­¤æè¿°è¿›è¡Œç›®æ ‡æ£€æµ‹"
                }),
                "load_florence2": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ˜¯å¦åŠ è½½Florence-2æ¨¡å‹ç”¨äºè‡ªåŠ¨ç”Ÿæˆå›¾åƒæè¿°ã€‚å½“promptå’Œexternal_captionéƒ½ä¸ºç©ºæ—¶ï¼Œå°†è‡ªåŠ¨æè¿°å›¾åƒå†…å®¹å¹¶è¿›è¡Œæ£€æµ‹"
                }),
                "min_area_ratio": ("FLOAT", {
                    "default": 0.002, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.0001, 
                    "tooltip": "æœ€å°é¢ç§¯æ¯”ä¾‹ï¼ˆç›¸å¯¹äºå›¾åƒæ€»é¢ç§¯ï¼‰ï¼Œç”¨äºè¿‡æ»¤å¤ªå°çš„åˆ†å‰²ç»“æœã€‚0.002è¡¨ç¤ºå å›¾åƒ0.2%ä»¥ä¸‹çš„åŒºåŸŸå°†è¢«è¿‡æ»¤æ‰"
                }),
                "max_area_ratio": ("FLOAT", {
                    "default": 0.2, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.01, 
                    "tooltip": "æœ€å¤§é¢ç§¯æ¯”ä¾‹ï¼ˆç›¸å¯¹äºå›¾åƒæ€»é¢ç§¯ï¼‰ï¼Œç”¨äºè¿‡æ»¤å¤ªå¤§çš„åˆ†å‰²ç»“æœã€‚0.2è¡¨ç¤ºå å›¾åƒ20%ä»¥ä¸Šçš„åŒºåŸŸå°†è¢«è¿‡æ»¤æ‰ï¼Œé¿å…èƒŒæ™¯è¯¯æ£€"
                }),
                "remaining_area_mask": ("MASK", {
                    "tooltip": "å¯é€‰ï¼šç”¨äºè®¡ç®—å‰©ä½™åŒºåŸŸçš„è¾“å…¥maskã€‚å½“æä¾›æ—¶ï¼ŒèŠ‚ç‚¹å°†åœ¨è¾“å‡ºobject_masksä¸­æ–°å¢ä¸€ä¸ªmaskï¼Œä»£è¡¨(è¾“å…¥mask âˆ© éå·²åˆ†å‰²åŒºåŸŸ)"
                }),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", "STRING", "STRING",)
    RETURN_NAMES = ("annotated_image", "object_masks", "detection_json", "object_names",)
    FUNCTION = "_process_image"
    CATEGORY = "ğŸ’ƒrDancer"
    OUTPUT_IS_LIST = (False, True, False, True)

    def _process_image(self, sam2_model: dict, grounding_dino_model: str, image: torch.Tensor, 
                      prompt: str = "", threshold: float = 0.3, iou_threshold: float = 0.5, external_caption: str = "", 
                      load_florence2: bool = True, min_area_ratio: float = 0.0001, max_area_ratio: float = 0.9,
                      remaining_area_mask: Optional[torch.Tensor] = None):
        
        # ä»SAM2æ¨¡å‹å­—å…¸ä¸­è·å–æ¨¡å‹å’Œè®¾å¤‡ä¿¡æ¯
        sam2_model_instance = sam2_model['model']
        device = sam2_model['device']
        
        # åŠ è½½GroundingDINOå’ŒFlorence2æ¨¡å‹
        lazy_load_grounding_dino_florence_models(grounding_dino_model, load_florence2)
        
        prompt_clean = prompt.strip() if prompt else ""
        external_caption_clean = external_caption.strip() if external_caption else ""
        
        annotated_images, object_masks_list, detection_jsons, final_object_names = [], [], [], []
        
        for i, img_tensor in enumerate(image):
            img_pil = tensor2pil(img_tensor).convert("RGB")
            
            # Determine processing mode
            current_detection_phrases = []
            detection_mode_info = ""

            if prompt_clean != "":
                # Mode 1: Direct prompt with GroundingDINO
                current_detection_phrases = [p.strip() for p in prompt_clean.split(',') if p.strip()]
                if not current_detection_phrases and prompt_clean:
                    current_detection_phrases = [prompt_clean.strip()]
                detection_mode_info = f"direct prompt list: {current_detection_phrases}"
                
            elif external_caption_clean != "":
                # Mode 2: Use external caption for grounding
                current_detection_phrases = [c.strip() for c in external_caption_clean.split(',') if c.strip()]
                if not current_detection_phrases and external_caption_clean:
                    current_detection_phrases = [external_caption_clean.strip()]
                detection_mode_info = f"external caption list: {current_detection_phrases}"
                
            else:
                # Mode 3: Generate caption with Florence-2, then use for grounding
                if load_florence2 and FLORENCE_MODEL is not None and FLORENCE_PROCESSOR is not None:
                    print(f"VVL_GroundingDinoSAM2: Image {i} - Generating caption with Florence-2.")
                    _, result_caption = run_florence_inference(
                        model=FLORENCE_MODEL,
                        processor=FLORENCE_PROCESSOR,
                        device=device,
                        image=img_pil,
                        task=FLORENCE_DETAILED_CAPTION_TASK
                    )
                    generated_caption = result_caption[FLORENCE_DETAILED_CAPTION_TASK]
                    current_detection_phrases = [c.strip() for c in generated_caption.split(',') if c.strip()]
                    if not current_detection_phrases and generated_caption:
                        current_detection_phrases = [generated_caption.strip()]
                    detection_mode_info = f"Florence-2 generated caption list: {current_detection_phrases}"
                else:
                    print("VVL_GroundingDinoSAM2: Florence-2 model not available, skipping caption generation.")
                    current_detection_phrases = []
                    detection_mode_info = "No detection phrases available"

            print(f"VVL_GroundingDinoSAM2: Image {i} - Mode: {detection_mode_info}")

            object_names = []
            all_boxes_list = []
            
            if not current_detection_phrases:
                print(f"VVL_GroundingDinoSAM2: Image {i} - No detection phrases to process.")
                boxes = torch.zeros((0,4))
            else:
                for phrase_idx, phrase in enumerate(current_detection_phrases):
                    boxes_single = groundingdino_predict(GROUNDING_DINO_MODEL, img_pil, phrase, threshold)
                    if boxes_single.shape[0] > 0:
                        all_boxes_list.append(boxes_single)
                        object_names.extend([phrase] * boxes_single.shape[0])
                
                if len(all_boxes_list) > 0:
                    boxes = torch.cat(all_boxes_list, dim=0)
                else:
                    boxes = torch.zeros((0,4))
            
            # Fallback logic if no boxes found with initial threshold
            if boxes.shape[0] == 0 and threshold > 0.15 and current_detection_phrases:
                fallback_thresh = max(0.1, threshold * 0.5)
                print(f"VVL_GroundingDinoSAM2: Image {i} - No boxes found with threshold {threshold}. Lowering to {fallback_thresh} and retrying.")
                
                all_boxes_list_fallback = []
                object_names_fallback = []

                for phrase_idx, phrase in enumerate(current_detection_phrases):
                    boxes_single_fallback = groundingdino_predict(GROUNDING_DINO_MODEL, img_pil, phrase, fallback_thresh)
                    if boxes_single_fallback.shape[0] > 0:
                        all_boxes_list_fallback.append(boxes_single_fallback)
                        object_names_fallback.extend([phrase] * boxes_single_fallback.shape[0])
                
                if len(all_boxes_list_fallback) > 0:
                    boxes = torch.cat(all_boxes_list_fallback, dim=0)
                    object_names = object_names_fallback
                else:
                    object_names = [] 
                    boxes = torch.zeros((0,4))

            print(f"VVL_GroundingDinoSAM2: Image {i} - Total boxes found for SAM2 input: {boxes.shape[0]}")
            if boxes.shape[0] > 0:
                print(f"VVL_GroundingDinoSAM2: Image {i} - Corresponding object names: {object_names}")

            # åº”ç”¨è¾¹ç•Œæ¡†å»é‡é€»è¾‘ï¼Œé¿å…é‡å¤åˆ†å‰²åŒä¸€ä¸ªå¯¹è±¡
            if boxes.shape[0] > 0:
                boxes_before_dedup = boxes.shape[0]
                boxes, object_names = remove_duplicate_boxes(boxes, object_names, iou_threshold)
                boxes_after_dedup = boxes.shape[0]
                if boxes_before_dedup != boxes_after_dedup:
                    print(f"VVL_GroundingDinoSAM2: Image {i} - Removed {boxes_before_dedup - boxes_after_dedup} duplicate boxes (IoU threshold: {iou_threshold})")
                    print(f"VVL_GroundingDinoSAM2: Image {i} - Final boxes for SAM2: {boxes_after_dedup}")

            if boxes.shape[0] == 0:
                print("VVL_GroundingDinoSAM2: No objects detected.")
                # Create empty results
                annotated_images.append(pil2tensor(img_pil))
                detection_jsons.append(json.dumps({
                    "image_width": img_pil.width,
                    "image_height": img_pil.height,
                    "objects": []
                }, ensure_ascii=False, indent=2))
                continue
            
            # Use SAM2 for segmentation
            output_images, output_masks, detections_with_masks = sam2_segment(sam2_model_instance, img_pil, boxes)
            
            # åº”ç”¨é¢ç§¯è¿‡æ»¤ï¼ˆå¦‚æœæœ‰åˆ†å‰²ç»“æœï¼‰
            if output_masks and (min_area_ratio > 0 or max_area_ratio < 1.0):
                print(f"VVL_GroundingDinoSAM2: Image {i} - åº”ç”¨é¢ç§¯è¿‡æ»¤ï¼ˆæœ€å°æ¯”ä¾‹: {min_area_ratio}, æœ€å¤§æ¯”ä¾‹: {max_area_ratio}ï¼‰")
                output_images, output_masks, detections_with_masks, object_names = filter_by_area(
                    output_images, output_masks, detections_with_masks, object_names, 
                    (img_pil.width, img_pil.height), min_area_ratio, max_area_ratio
                )
                
                # å¦‚æœæ‰€æœ‰ç»“æœéƒ½è¢«é¢ç§¯è¿‡æ»¤æ‰äº†
                if not output_masks:
                    print(f"VVL_GroundingDinoSAM2: Image {i} - æ‰€æœ‰åˆ†å‰²ç»“æœéƒ½è¢«é¢ç§¯è¿‡æ»¤æ‰äº†")
                    annotated_images.append(pil2tensor(img_pil))
                    detection_jsons.append(json.dumps({
                        "image_width": img_pil.width,
                        "image_height": img_pil.height,
                        "objects": []
                    }, ensure_ascii=False, indent=2))
                    continue
            
            # å¤„ç† remaining_area_maskï¼Œç”Ÿæˆå‰©ä½™åŒºåŸŸmask
            if remaining_area_mask is not None:
                # å–å¾—å½“å‰æ‰¹æ¬¡å¯¹åº”çš„è¾“å…¥mask
                if remaining_area_mask.ndim == 4:
                    cur_input_mask = remaining_area_mask[i]
                else:
                    cur_input_mask = remaining_area_mask

                # è½¬æ¢ä¸º (H, W) çš„å¸ƒå°”æ•°ç»„
                input_mask_np = cur_input_mask.cpu().numpy()
                if input_mask_np.ndim == 3:
                    # å¤„ç†å½¢çŠ¶ä¸º (C, H, W) æˆ– (H, W, C)
                    if input_mask_np.shape[0] == 1:  # (1, H, W)
                        input_mask_np = input_mask_np[0]
                    else:  # (H, W, 1) æˆ–å…¶ä»–
                        input_mask_np = input_mask_np[:, :, 0]
                input_mask_bool = np.squeeze(input_mask_np) > 0.5

                # åˆå¹¶å·²æœ‰çš„æ‰€æœ‰mask
                combined_existing = np.zeros_like(input_mask_bool, dtype=bool)
                for m_tensor in output_masks:
                    m_np = m_tensor.cpu().numpy()
                    if m_np.ndim == 3:
                        if m_np.shape[0] == 1:
                            m_np = m_np[0]
                        else:
                            m_np = m_np[:, :, 0]
                    combined_existing |= (m_np > 0.5)

                # è®¡ç®—å‰©ä½™åŒºåŸŸ (è¾“å…¥mask äº¤ éå·²åˆ†å‰²åŒºåŸŸ)
                remain_bool = np.logical_and(input_mask_bool, np.logical_not(combined_existing))

                if np.sum(remain_bool) > 0:
                    # æ¸…ç†é›¶ç¢åŒºåŸŸï¼Œåªä¿ç•™æœ€å¤§çš„è¿é€šåŸŸ
                    remain_uint8 = (remain_bool * 255).astype(np.uint8)
                    remain_cleaned = remove_small_regions(remain_uint8, keep_largest_n=1)
                    remain_bool_cleaned = remain_cleaned > 127
                    
                    # å¦‚æœæ¸…ç†åè¿˜æœ‰åŒºåŸŸå­˜åœ¨ï¼Œåˆ™ç»§ç»­å¤„ç†
                    if np.sum(remain_bool_cleaned) > 0:
                        # ç”Ÿæˆmask tensor
                        remain_mask_pil = Image.fromarray(remain_cleaned).convert("L")
                        remain_mask_tensor = pil2tensor(remain_mask_pil)
                        output_masks.append(remain_mask_tensor)

                        # ç”Ÿæˆå¯¹åº”çš„masked image
                        img_np_full = np.array(img_pil)
                        img_np_copy = copy.deepcopy(img_np_full)
                        if len(img_np_copy.shape) == 3:
                            img_np_copy[~remain_bool_cleaned] = np.array([0, 0, 0])
                        else:
                            img_np_copy[~remain_bool_cleaned] = np.array([0, 0, 0, 0])
                        remain_image_pil = Image.fromarray(img_np_copy)
                        output_images.append(pil2tensor(remain_image_pil.convert("RGB")))

                        # è®¡ç®—bboxï¼ˆåŸºäºæ¸…ç†åçš„maskï¼‰
                        ys, xs = np.where(remain_bool_cleaned)
                        x_min, x_max = int(xs.min()), int(xs.max())
                        y_min, y_max = int(ys.min()), int(ys.max())
                        bbox_tensor = torch.tensor([x_min, y_min, x_max, y_max], dtype=torch.float32)

                        # æ›´æ–° detections_with_masks
                        if detections_with_masks is None:
                            detections_with_masks = sv.Detections(xyxy=bbox_tensor.unsqueeze(0), mask=np.asarray([remain_bool_cleaned]))
                        else:
                            # æ ¹æ®ç°æœ‰ xyxy çš„æ•°æ®ç±»å‹å†³å®šæ‹¼æ¥æ–¹å¼ï¼Œé¿å… numpy ä¸ tensor å†²çª
                            if hasattr(detections_with_masks, 'xyxy') and detections_with_masks.xyxy is not None:
                                if isinstance(detections_with_masks.xyxy, np.ndarray):
                                    bbox_np = bbox_tensor.cpu().numpy()[None, :]
                                    detections_with_masks.xyxy = np.concatenate([detections_with_masks.xyxy, bbox_np], axis=0)
                                else:
                                    detections_with_masks.xyxy = torch.cat([detections_with_masks.xyxy, bbox_tensor.unsqueeze(0)], dim=0)
                            else:
                                # åˆå§‹ä¸ºç©ºæ—¶æ²¿ç”¨ bbox çš„ç±»å‹
                                detections_with_masks.xyxy = bbox_tensor.unsqueeze(0)
                            # mask
                            if hasattr(detections_with_masks, 'mask') and detections_with_masks.mask is not None:
                                detections_with_masks.mask = np.concatenate([detections_with_masks.mask, remain_bool_cleaned[None, :, :]], axis=0)
                            else:
                                detections_with_masks.mask = np.asarray([remain_bool_cleaned])
                        
                        # è¿½åŠ åç§°
                        object_names.append("remaining_area")

            # å°†æœ€ç»ˆçš„å¯¹è±¡åç§°æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            final_object_names.extend(object_names)
            
            # ä½¿ç”¨supervisionåº“çš„æ ‡æ³¨å™¨æ¥æ ‡æ³¨å›¾åƒ
            if len(object_names) > 0 and detections_with_masks is not None:
                # åˆ›å»ºæ ‡ç­¾åˆ—è¡¨ï¼Œç¡®ä¿é•¿åº¦ä¸æ£€æµ‹ç»“æœåŒ¹é…
                labels = []
                for j in range(len(detections_with_masks)):
                    if j < len(object_names):
                        labels.append(object_names[j])
                    else:
                        labels.append(f"object_{j+1}")
                
                # è®¾ç½®detectionsçš„dataå­—å…¸æ¥å­˜å‚¨æ ‡ç­¾
                if not hasattr(detections_with_masks, 'data') or detections_with_masks.data is None:
                    detections_with_masks.data = {}
                detections_with_masks.data['class_name'] = labels
                
                # ä½¿ç”¨ä¸app.pyç›¸åŒçš„annotate_imageå‡½æ•°
                annotated_img = annotate_image(img_pil, detections_with_masks)
                annotated_images.append(pil2tensor(annotated_img))
            else:
                # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°å¯¹è±¡ï¼Œåˆ™è¿”å›åŸå§‹å›¾åƒ
                annotated_images.append(pil2tensor(img_pil))
            
            # Add masks to the list
            if output_masks:
                object_masks_list.extend(output_masks)
            
            # Create detection JSON
            detection_json = {
                "image_width": img_pil.width,
                "image_height": img_pil.height,
                "objects": []
            }
            
            # ä½¿ç”¨è¿‡æ»¤åçš„æ£€æµ‹ç»“æœæ¥ç”ŸæˆJSON
            if detections_with_masks is not None and hasattr(detections_with_masks, 'xyxy') and detections_with_masks.xyxy is not None:
                for j, bbox in enumerate(detections_with_masks.xyxy):
                    bbox_2d = [int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])]
                    detection_json["objects"].append({
                        "name": object_names[j] if j < len(object_names) else f"object_{j+1}",
                        "bbox_2d": bbox_2d
                    })
            
            # Format JSON with single-line bbox_2d
            json_str = json.dumps(detection_json, ensure_ascii=False, indent=2)
            json_str = re.sub(r'"bbox_2d":\s*\[\s*(\d+),\s*(\d+),\s*(\d+),\s*(\d+)\s*\]', 
                             r'"bbox_2d": [\1,\2,\3,\4]', json_str)
            detection_jsons.append(json_str)
        
        # Stack results
        annotated_images_stacked = torch.stack(annotated_images) if annotated_images else torch.empty(0)
        final_detection_json = detection_jsons[0] if detection_jsons else "{}"
        
        return (annotated_images_stacked, object_masks_list, final_detection_json, final_object_names)

# Node registration
NODE_CLASS_MAPPINGS = {
    "VVL_GroundingDinoSAM2": VVL_GroundingDinoSAM2
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VVL_GroundingDinoSAM2": "VVL GroundingDINO + SAM2"
}

__all__ = ["NODE_CLASS_MAPPINGS", "NODE_DISPLAY_NAME_MAPPINGS"] 