import torch
from PIL import Image, ImageDraw
import numpy as np
import os
import json
import re
import copy
import supervision as sv
from typing import Tuple, Optional, Any, List

try:
    from florence_sam_processor import process_image
    from utils.sam import model_to_config_map as sam_model_to_config_map
    from utils.sam import load_sam_image_model, run_sam_inference
    from utils.florence import load_florence_model, run_florence_inference, \
        FLORENCE_OPEN_VOCABULARY_DETECTION_TASK, FLORENCE_DETAILED_CAPTION_TASK, FLORENCE_CAPTION_TO_PHRASE_GROUNDING_TASK
    from utils.modes import IMAGE_INFERENCE_MODES, IMAGE_OPEN_VOCABULARY_DETECTION_MODE, IMAGE_CAPTION_GROUNDING_MASKS_MODE, VIDEO_INFERENCE_MODES
    from mask_cleaner import remove_small_regions
    from grounding_dino_sam2 import (
        tensor2pil, pil2tensor, load_groundingdino_model, groundingdino_predict,
        annotate_image, resolve_duplicate_names, verify_data_consistency,
        calculate_iou, remove_duplicate_boxes, calculate_mask_containment_ratio,
        remove_duplicate_masks_by_containment, filter_by_area, sam2_segment,
        groundingdino_model_list
    )
except ImportError:
    # We're running as a module
    from .florence_sam_processor import process_image
    from .utils.sam import model_to_config_map as sam_model_to_config_map
    from .utils.sam import load_sam_image_model, run_sam_inference
    from .utils.florence import load_florence_model, run_florence_inference, \
        FLORENCE_OPEN_VOCABULARY_DETECTION_TASK, FLORENCE_DETAILED_CAPTION_TASK, FLORENCE_CAPTION_TO_PHRASE_GROUNDING_TASK
    from .utils.modes import IMAGE_INFERENCE_MODES, IMAGE_OPEN_VOCABULARY_DETECTION_MODE, IMAGE_CAPTION_GROUNDING_MASKS_MODE, VIDEO_INFERENCE_MODES
    from .mask_cleaner import remove_small_regions
    from .grounding_dino_sam2 import (
        tensor2pil, pil2tensor, load_groundingdino_model, groundingdino_predict,
        annotate_image, resolve_duplicate_names, verify_data_consistency,
        calculate_iou, remove_duplicate_boxes, calculate_mask_containment_ratio,
        remove_duplicate_masks_by_containment, filter_by_area, sam2_segment,
        groundingdino_model_list
    )

import logging
import comfy.model_management

logger = logging.getLogger('vvl_GroundingDinoSAM2_Video')

# å¯¼å…¥åŸå§‹æ¨¡å—çš„å…¨å±€å˜é‡å’Œå‡½æ•°ï¼Œè€Œä¸æ˜¯é‡æ–°å®šä¹‰
try:
    import grounding_dino_sam2 as gd_sam2_module
except ImportError:
    from . import grounding_dino_sam2 as gd_sam2_module


class VVL_GroundingDinoSAM2_VideoSequence:
    """
    åŸºäºGroundingDINO + SAM2çš„è§†é¢‘åºåˆ—å¤„ç†èŠ‚ç‚¹
    è¾“å…¥å›¾ç‰‡åºåˆ—ï¼Œè¾“å‡ºå¸¦æœ‰åˆ†å‰²ç»“æœçš„å›¾ç‰‡åºåˆ—
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        grounding_dino_models = list(groundingdino_model_list.keys())
        
        return {
            "required": {
                "sam2_model": ("VVL_SAM2_MODEL", {"tooltip": "SAM2åˆ†å‰²æ¨¡å‹ï¼Œç”¨äºå¯¹æ£€æµ‹åˆ°çš„å¯¹è±¡è¿›è¡Œç²¾ç¡®åˆ†å‰²"}),
                "grounding_dino_model": (grounding_dino_models, {
                    "default": grounding_dino_models[0],
                    "tooltip": "GroundingDINOç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œç”¨äºæ ¹æ®æ–‡æœ¬æç¤ºæ£€æµ‹å›¾åƒä¸­çš„å¯¹è±¡"
                }),
                "image_sequence": ("IMAGE", {"tooltip": "è¾“å…¥çš„å›¾åƒåºåˆ—ï¼Œç”¨äºè§†é¢‘å¤„ç†ã€‚æ”¯æŒæ‰¹é‡å›¾åƒ"}),
                "prompt": ("STRING", {
                    "default": "",
                    "tooltip": "ç›®æ ‡æ£€æµ‹çš„æ–‡æœ¬æç¤ºè¯ï¼Œç”¨é€—å·åˆ†éš”å¤šä¸ªå¯¹è±¡ï¼Œå¦‚'person,car,dog'ã€‚ç•™ç©ºæ—¶å°†ä½¿ç”¨Florence-2è‡ªåŠ¨ç”Ÿæˆæè¿°"
                }),
                "threshold": ("FLOAT", {
                    "default": 0.3, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.01,
                    "tooltip": "ç›®æ ‡æ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œå»ºè®®èŒƒå›´0.2-0.5"
                }),
                "iou_threshold": ("FLOAT", {
                    "default": 0.5, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.01,
                    "tooltip": "IoUé˜ˆå€¼ç”¨äºå»é™¤é‡å¤æ£€æµ‹æ¡†ï¼Œå»ºè®®0.3-0.7"
                }),
                "consistency_threshold": ("FLOAT", {
                    "default": 0.7, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.05,
                    "tooltip": "å¸§é—´ä¸€è‡´æ€§é˜ˆå€¼ï¼Œç”¨äºä¿æŒç›¸åŒå¯¹è±¡åœ¨ä¸åŒå¸§ä¸­çš„æ ‡è¯†ä¸€è‡´ã€‚å€¼è¶Šé«˜è¦æ±‚è¶Šä¸¥æ ¼"
                }),
            },
            "optional": {
                "external_caption": ("STRING", {
                    "multiline": True, 
                    "default": "",
                    "tooltip": "å¤–éƒ¨æä¾›çš„å›¾åƒæè¿°æ–‡æœ¬ï¼Œç”¨é€—å·åˆ†éš”å¤šä¸ªå¯¹è±¡"
                }),
                "load_florence2": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ˜¯å¦åŠ è½½Florence-2æ¨¡å‹ç”¨äºè‡ªåŠ¨ç”Ÿæˆå›¾åƒæè¿°"
                }),
                "min_area_ratio": ("FLOAT", {
                    "default": 0.002, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.0001, 
                    "tooltip": "æœ€å°é¢ç§¯æ¯”ä¾‹ï¼Œç”¨äºè¿‡æ»¤å¤ªå°çš„åˆ†å‰²ç»“æœ"
                }),
                "max_area_ratio": ("FLOAT", {
                    "default": 0.2, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.01, 
                    "tooltip": "æœ€å¤§é¢ç§¯æ¯”ä¾‹ï¼Œç”¨äºè¿‡æ»¤å¤ªå¤§çš„åˆ†å‰²ç»“æœ"
                }),
                "use_reference_frame": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "æ˜¯å¦ä½¿ç”¨ç¬¬ä¸€å¸§ä½œä¸ºå‚è€ƒå¸§æ¥ä¿æŒå¯¹è±¡æ ‡è¯†çš„ä¸€è‡´æ€§"
                }),
                "mask_containment_threshold": ("FLOAT", {
                    "default": 0.8, 
                    "min": 0, 
                    "max": 1.0, 
                    "step": 0.05,
                    "tooltip": "åŸºäºå®é™…maskå½¢çŠ¶çš„åŒ…å«é˜ˆå€¼ï¼Œç”¨äºç§»é™¤è¢«å…¶ä»–maskåŒ…å«çš„é‡å¤åˆ†å‰²ã€‚è®¾ä¸º0æ—¶ç¦ç”¨æ­¤åŠŸèƒ½"
                }),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", "STRING", "STRING",)
    RETURN_NAMES = ("annotated_sequence", "sequence_masks", "sequence_info", "sequence_object_names",)
    FUNCTION = "_process_video_sequence"
    CATEGORY = "ğŸ’ƒrDancer"
    OUTPUT_IS_LIST = (False, True, False, True)

    def calculate_bbox_similarity(self, bbox1, bbox2, image_size):
        """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„ç›¸ä¼¼åº¦"""
        # å½’ä¸€åŒ–åæ ‡
        w, h = image_size
        bbox1_norm = [bbox1[0]/w, bbox1[1]/h, bbox1[2]/w, bbox1[3]/h]
        bbox2_norm = [bbox2[0]/w, bbox2[1]/h, bbox2[2]/w, bbox2[3]/h]
        
        # è®¡ç®—ä¸­å¿ƒç‚¹è·ç¦»
        center1 = [(bbox1_norm[0] + bbox1_norm[2])/2, (bbox1_norm[1] + bbox1_norm[3])/2]
        center2 = [(bbox2_norm[0] + bbox2_norm[2])/2, (bbox2_norm[1] + bbox2_norm[3])/2]
        center_dist = ((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)**0.5
        
        # è®¡ç®—é¢ç§¯ç›¸ä¼¼åº¦
        area1 = (bbox1_norm[2] - bbox1_norm[0]) * (bbox1_norm[3] - bbox1_norm[1])
        area2 = (bbox2_norm[2] - bbox2_norm[0]) * (bbox2_norm[3] - bbox2_norm[1])
        area_similarity = min(area1, area2) / max(area1, area2) if max(area1, area2) > 0 else 0
        
        # è®¡ç®—IoU
        iou = calculate_iou(bbox1, bbox2)
        
        # ç»¼åˆç›¸ä¼¼åº¦ (IoUæƒé‡æœ€é«˜)
        similarity = 0.6 * iou + 0.2 * area_similarity + 0.2 * (1 - min(center_dist, 1.0))
        return similarity

    def match_objects_across_frames(self, reference_detections, current_detections, 
                                   reference_names, current_names, image_size, threshold=0.5):
        """åœ¨å¸§é—´åŒ¹é…å¯¹è±¡ï¼Œä¿æŒæ ‡è¯†ä¸€è‡´æ€§"""
        if reference_detections is None or current_detections is None:
            return current_names, list(range(len(current_names))) if current_names else []
        
        ref_boxes = reference_detections.xyxy if hasattr(reference_detections, 'xyxy') else []
        cur_boxes = current_detections.xyxy if hasattr(current_detections, 'xyxy') else []
        
        if len(ref_boxes) == 0 or len(cur_boxes) == 0:
            return current_names, list(range(len(current_names))) if current_names else []
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = np.zeros((len(ref_boxes), len(cur_boxes)))
        for i, ref_box in enumerate(ref_boxes):
            for j, cur_box in enumerate(cur_boxes):
                similarity_matrix[i, j] = self.calculate_bbox_similarity(ref_box, cur_box, image_size)
        
        # åŒ¹é…å¯¹è±¡ï¼ˆä½¿ç”¨è´ªå¿ƒç®—æ³•ï¼‰
        matched_names = current_names.copy()
        matched_indices = list(range(len(current_names)))
        used_ref_indices = set()
        
        # æŒ‰ç›¸ä¼¼åº¦ä»é«˜åˆ°ä½æ’åº
        pairs = []
        for i in range(similarity_matrix.shape[0]):
            for j in range(similarity_matrix.shape[1]):
                if similarity_matrix[i, j] > threshold:
                    pairs.append((i, j, similarity_matrix[i, j]))
        
        pairs.sort(key=lambda x: x[2], reverse=True)
        
        # æ‰§è¡ŒåŒ¹é…
        for ref_idx, cur_idx, similarity in pairs:
            if ref_idx not in used_ref_indices and cur_idx < len(matched_names):
                # ä½¿ç”¨å‚è€ƒå¸§çš„å¯¹è±¡åç§°
                if ref_idx < len(reference_names):
                    matched_names[cur_idx] = reference_names[ref_idx]
                    matched_indices[cur_idx] = ref_idx
                    used_ref_indices.add(ref_idx)
        
        return matched_names, matched_indices

    def _process_video_sequence(self, sam2_model: dict, grounding_dino_model: str, 
                               image_sequence: torch.Tensor, prompt: str = "", 
                               threshold: float = 0.3, iou_threshold: float = 0.5, 
                               consistency_threshold: float = 0.7, external_caption: str = "", 
                               load_florence2: bool = True, min_area_ratio: float = 0.002, 
                               max_area_ratio: float = 0.2, use_reference_frame: bool = True,
                               mask_containment_threshold: float = 0.8):
        
        # åŠ è½½æ¨¡å‹
        gd_sam2_module.lazy_load_grounding_dino_florence_models(grounding_dino_model, load_florence2)
        
        # ä»SAM2æ¨¡å‹å­—å…¸ä¸­è·å–æ¨¡å‹å’Œè®¾å¤‡ä¿¡æ¯
        sam2_model_instance = sam2_model['model']
        device = sam2_model['device']
        
        sequence_length = image_sequence.shape[0]
        print(f"VVL_GroundingDinoSAM2_VideoSequence: å¤„ç†è§†é¢‘åºåˆ—ï¼Œå…± {sequence_length} å¸§")
        
        annotated_images = []
        sequence_masks_list = []
        sequence_object_names = []
        
        # å‚è€ƒå¸§ä¿¡æ¯ï¼ˆç”¨äºä¿æŒå¯¹è±¡ä¸€è‡´æ€§ï¼‰
        reference_detections = None
        reference_names = []
        reference_frame_idx = 0
        
        # åºåˆ—ä¿¡æ¯ç»Ÿè®¡
        total_objects_detected = 0
        frames_with_objects = 0
        
        for i, img_tensor in enumerate(image_sequence):
            img_pil = tensor2pil(img_tensor).convert("RGB")
            print(f"VVL_GroundingDinoSAM2_VideoSequence: å¤„ç†ç¬¬ {i+1}/{sequence_length} å¸§")
            
            # ä½¿ç”¨ä¸å•å¸§å¤„ç†ç›¸åŒçš„é€»è¾‘ç¡®å®šæ£€æµ‹çŸ­è¯­
            prompt_clean = prompt.strip() if prompt else ""
            external_caption_clean = external_caption.strip() if external_caption else ""
            
            current_detection_phrases = []
            
            if prompt_clean:
                current_detection_phrases = [p.strip() for p in prompt_clean.split(',') if p.strip()]
                if not current_detection_phrases and prompt_clean:
                    current_detection_phrases = [prompt_clean.strip()]
            elif external_caption_clean:
                current_detection_phrases = [c.strip() for c in external_caption_clean.split(',') if c.strip()]
                if not current_detection_phrases and external_caption_clean:
                    current_detection_phrases = [external_caption_clean.strip()]
            else:
                # ä»…åœ¨ç¬¬ä¸€å¸§æˆ–å‚è€ƒå¸§ç”Ÿæˆæè¿°ï¼Œåç»­å¸§å¤ç”¨
                if i == 0 and load_florence2 and gd_sam2_module.FLORENCE_MODEL is not None and gd_sam2_module.FLORENCE_PROCESSOR is not None:
                    print(f"VVL_GroundingDinoSAM2_VideoSequence: ç¬¬ {i+1} å¸§ - ä½¿ç”¨Florence-2ç”Ÿæˆæè¿°")
                    _, result_caption = run_florence_inference(
                        model=gd_sam2_module.FLORENCE_MODEL,
                        processor=gd_sam2_module.FLORENCE_PROCESSOR,
                        device=device,
                        image=img_pil,
                        task=FLORENCE_DETAILED_CAPTION_TASK
                    )
                    generated_caption = result_caption[FLORENCE_DETAILED_CAPTION_TASK]
                    current_detection_phrases = [c.strip() for c in generated_caption.split(',') if c.strip()]
                    if not current_detection_phrases and generated_caption:
                        current_detection_phrases = [generated_caption.strip()]
                elif i > 0 and reference_names:
                    # åç»­å¸§ä½¿ç”¨å‚è€ƒå¸§çš„å¯¹è±¡åç§°
                    current_detection_phrases = list(set(reference_names))
            
            # GroundingDINOæ£€æµ‹
            object_names = []
            all_boxes_list = []
            
            if current_detection_phrases:
                # éªŒè¯æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½
                if gd_sam2_module.GROUNDING_DINO_MODEL is None:
                    print(f"VVL_GroundingDinoSAM2_VideoSequence: é”™è¯¯ - GROUNDING_DINO_MODELæ˜¯None!")
                    raise ValueError("GroundingDINOæ¨¡å‹æœªæ­£ç¡®åŠ è½½")
                
                for phrase in current_detection_phrases:
                    print(f"VVL_GroundingDinoSAM2_VideoSequence: ç¬¬ {i+1} å¸§ - æ£€æµ‹çŸ­è¯­: '{phrase}'")
                    boxes_single = groundingdino_predict(gd_sam2_module.GROUNDING_DINO_MODEL, img_pil, phrase, threshold)
                    if boxes_single.shape[0] > 0:
                        all_boxes_list.append(boxes_single)
                        object_names.extend([phrase] * boxes_single.shape[0])
                        print(f"VVL_GroundingDinoSAM2_VideoSequence: ç¬¬ {i+1} å¸§ - çŸ­è¯­ '{phrase}' æ£€æµ‹åˆ° {boxes_single.shape[0]} ä¸ªæ¡†")
                    else:
                        print(f"VVL_GroundingDinoSAM2_VideoSequence: ç¬¬ {i+1} å¸§ - çŸ­è¯­ '{phrase}' æœªæ£€æµ‹åˆ°å¯¹è±¡")
                
                if len(all_boxes_list) > 0:
                    boxes = torch.cat(all_boxes_list, dim=0)
                else:
                    boxes = torch.zeros((0,4))
            else:
                boxes = torch.zeros((0,4))
            
            # å»é‡æ£€æµ‹æ¡†
            if boxes.shape[0] > 0:
                boxes, object_names = remove_duplicate_boxes(boxes, object_names, iou_threshold)
            
            if boxes.shape[0] == 0:
                print(f"VVL_GroundingDinoSAM2_VideoSequence: ç¬¬ {i+1} å¸§ - æœªæ£€æµ‹åˆ°å¯¹è±¡")
                annotated_images.append(pil2tensor(img_pil))
                continue
            
            # SAM2åˆ†å‰²
            output_images, output_masks, detections_with_masks = sam2_segment(sam2_model_instance, img_pil, boxes)
            
            # åŸºäºå®é™…maskå½¢çŠ¶å»é™¤é‡å¤åˆ†å‰²ï¼ˆåœ¨é¢ç§¯è¿‡æ»¤ä¹‹å‰è¿›è¡Œï¼‰
            if output_masks and len(output_masks) > 1 and mask_containment_threshold > 0:
                print(f"VVL_GroundingDinoSAM2_VideoSequence: ç¬¬ {i+1} å¸§ - åº”ç”¨åŸºäºmaskçš„å»é‡é€»è¾‘ (é˜ˆå€¼: {mask_containment_threshold})")
                output_images, output_masks, detections_with_masks, object_names = remove_duplicate_masks_by_containment(
                    output_images, output_masks, detections_with_masks, object_names, containment_threshold=mask_containment_threshold
                )
            
            # é¢ç§¯è¿‡æ»¤
            if output_masks and (min_area_ratio > 0 or max_area_ratio < 1.0):
                output_images, output_masks, detections_with_masks, object_names = filter_by_area(
                    output_images, output_masks, detections_with_masks, object_names, 
                    (img_pil.width, img_pil.height), min_area_ratio, max_area_ratio
                )
            
            if not output_masks:
                print(f"VVL_GroundingDinoSAM2_VideoSequence: ç¬¬ {i+1} å¸§ - æ‰€æœ‰åˆ†å‰²ç»“æœè¢«è¿‡æ»¤")
                annotated_images.append(pil2tensor(img_pil))
                continue
            
            # å¸§é—´å¯¹è±¡åŒ¹é…ï¼ˆä¿æŒä¸€è‡´æ€§ï¼‰
            if use_reference_frame and reference_detections is not None:
                matched_names, matched_indices = self.match_objects_across_frames(
                    reference_detections, detections_with_masks, reference_names, 
                    object_names, (img_pil.width, img_pil.height), consistency_threshold
                )
                object_names = matched_names
                matched_count = len([idx for idx in matched_indices if idx < len(reference_names)])
                print(f"VVL_GroundingDinoSAM2_VideoSequence: ç¬¬ {i+1} å¸§ - å¯¹è±¡åŒ¹é…å®Œæˆï¼ŒåŒ¹é…åˆ° {matched_count} ä¸ªå·²çŸ¥å¯¹è±¡")
            
            # è®¾ç½®å‚è€ƒå¸§
            if use_reference_frame and (reference_detections is None or i == reference_frame_idx):
                reference_detections = detections_with_masks
                reference_names = object_names.copy()
                print(f"VVL_GroundingDinoSAM2_VideoSequence: ç¬¬ {i+1} å¸§è®¾ä¸ºå‚è€ƒå¸§ï¼ŒåŒ…å« {len(reference_names)} ä¸ªå¯¹è±¡")
            
            # è§£å†³å¯¹è±¡åç§°é‡å¤é—®é¢˜
            object_names = resolve_duplicate_names(object_names)
            
            # éªŒè¯æ•°æ®ä¸€è‡´æ€§
            verify_data_consistency(object_names, detections_with_masks, output_masks, image_index=i)
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_objects_detected += len(object_names)
            frames_with_objects += 1
            
            # å›¾åƒæ ‡æ³¨
            if len(object_names) > 0 and detections_with_masks is not None:
                labels = object_names[:len(detections_with_masks)]
                
                if not hasattr(detections_with_masks, 'data') or detections_with_masks.data is None:
                    detections_with_masks.data = {}
                detections_with_masks.data['class_name'] = labels
                
                annotated_img = annotate_image(img_pil, detections_with_masks)
                annotated_images.append(pil2tensor(annotated_img))
            else:
                annotated_images.append(pil2tensor(img_pil))
            
            # æ·»åŠ maskså’Œå¯¹è±¡åç§°
            if output_masks:
                sequence_masks_list.extend(output_masks)
            sequence_object_names.extend(object_names)
            
            print(f"VVL_GroundingDinoSAM2_VideoSequence: ç¬¬ {i+1} å¸§å¤„ç†å®Œæˆï¼Œæ£€æµ‹åˆ° {len(object_names)} ä¸ªå¯¹è±¡")
        
        # ç”Ÿæˆåºåˆ—ä¿¡æ¯
        avg_objects_per_frame = total_objects_detected / frames_with_objects if frames_with_objects > 0 else 0
        unique_object_names = list(set(sequence_object_names))
        
        sequence_info = {
            "total_frames": sequence_length,
            "frames_with_objects": frames_with_objects,
            "total_objects_detected": total_objects_detected,
            "average_objects_per_frame": round(avg_objects_per_frame, 2),
            "unique_object_types": len(unique_object_names),
            "object_types": unique_object_names,
            "processing_settings": {
                "grounding_dino_model": grounding_dino_model,
                "threshold": threshold,
                "iou_threshold": iou_threshold,
                "consistency_threshold": consistency_threshold,
                "use_reference_frame": use_reference_frame,
                "mask_containment_threshold": mask_containment_threshold,
                "min_area_ratio": min_area_ratio,
                "max_area_ratio": max_area_ratio
            }
        }
        
        sequence_info_json = json.dumps(sequence_info, ensure_ascii=False, indent=2)
        
        # å †å ç»“æœ
        annotated_images_stacked = torch.stack(annotated_images) if annotated_images else torch.empty(0)
        
        print(f"VVL_GroundingDinoSAM2_VideoSequence: åºåˆ—å¤„ç†å®Œæˆ")
        print(f"  - æ€»å¸§æ•°: {sequence_length}")
        print(f"  - æœ‰å¯¹è±¡çš„å¸§æ•°: {frames_with_objects}")
        print(f"  - æ£€æµ‹åˆ°çš„å¯¹è±¡æ€»æ•°: {total_objects_detected}")
        print(f"  - å¹³å‡æ¯å¸§å¯¹è±¡æ•°: {avg_objects_per_frame:.2f}")
        print(f"  - å”¯ä¸€å¯¹è±¡ç±»å‹: {len(unique_object_names)}")
        print(f"  - å¯¹è±¡ç±»å‹: {unique_object_names}")
        
        return (annotated_images_stacked, sequence_masks_list, sequence_info_json, sequence_object_names)


# Node registration
NODE_CLASS_MAPPINGS = {
    "VVL_GroundingDinoSAM2_VideoSequence": VVL_GroundingDinoSAM2_VideoSequence
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VVL_GroundingDinoSAM2_VideoSequence": "VVL GroundingDINO + SAM2 è§†é¢‘åºåˆ—"
}

__all__ = ["NODE_CLASS_MAPPINGS", "NODE_DISPLAY_NAME_MAPPINGS"] 