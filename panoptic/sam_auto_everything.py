import torch
import numpy as np
from PIL import Image
from typing import List, Tuple
import json
import supervision as sv  # æ·»åŠ supervisionåº“ç”¨äºå›¾åƒæ ‡æ³¨
import os
import folder_paths
import comfy.model_management
from torch.hub import download_url_to_file
from urllib.parse import urlparse
import logging

try:
    from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
    # å°è¯•å¯¼å…¥ sam_hq çš„æ¨¡å‹æ³¨å†Œè¡¨
    try:
        from sam_hq.build_sam_hq import sam_model_registry as sam_hq_model_registry
        # åˆå¹¶ sam_hq åˆ° sam_model_registry
        sam_model_registry.update(sam_hq_model_registry)
        SAM_HQ_AVAILABLE = True
    except ImportError:
        SAM_HQ_AVAILABLE = False
        print("Warning: sam_hq æ¨¡å—æœªæ‰¾åˆ°ï¼ŒSAM-HQ æ¨¡å‹å°†ä¸å¯ç”¨")
except ImportError as e:
    raise ImportError("segment_anything åº“æœªå®‰è£…ï¼Œè¯·æ‰§è¡Œ pip install git+https://github.com/facebookresearch/segment-anything.git") from e

from ..utils.panoptic_utils import mask_to_bbox, build_detection_json

logger = logging.getLogger('ComfyUI_VVL_SAM2')

# SAM1 æ¨¡å‹é…ç½®
sam_model_dir_name = "sams"
sam_model_list = {
    "sam_vit_h (2.56GB)": {
        "model_url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
    },
    "sam_vit_l (1.25GB)": {
        "model_url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth"
    },
    "sam_vit_b (375MB)": {
        "model_url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
    },
    "sam_hq_vit_h (2.57GB)": {
        "model_url": "https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_h.pth"
    },
    "sam_hq_vit_l (1.25GB)": {
        "model_url": "https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_l.pth"
    },
    "sam_hq_vit_b (379MB)": {
        "model_url": "https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_b.pth"
    },
}

def list_sam_model():
    """åˆ—å‡ºå¯ç”¨çš„ SAM1 æ¨¡å‹"""
    available_models = []
    for model_name in sam_model_list.keys():
        # å¦‚æœæ˜¯ SAM-HQ æ¨¡å‹ä½† SAM-HQ ä¸å¯ç”¨ï¼Œåˆ™è·³è¿‡
        if 'sam_hq' in model_name and not SAM_HQ_AVAILABLE:
            continue
        available_models.append(model_name)
    return available_models

def get_local_filepath(url, dirname, local_file_name=None):
    """è·å–æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸‹è½½"""
    if not local_file_name:
        parsed_url = urlparse(url)
        local_file_name = os.path.basename(parsed_url.path)

    destination = folder_paths.get_full_path(dirname, local_file_name)
    if destination:
        logger.warn(f'using extra model: {destination}')
        return destination

    folder = os.path.join(folder_paths.models_dir, dirname)
    if not os.path.exists(folder):
        os.makedirs(folder)

    destination = os.path.join(folder, local_file_name)
    if not os.path.exists(destination):
        logger.warn(f'downloading {url} to {destination}')
        download_url_to_file(url, destination)
    return destination

def load_sam_model(model_name):
    """åŠ è½½ SAM1 æ¨¡å‹"""
    sam_checkpoint_path = get_local_filepath(
        sam_model_list[model_name]["model_url"], sam_model_dir_name)
    model_file_name = os.path.basename(sam_checkpoint_path)
    model_type = model_file_name.split('.')[0]
    if 'hq' not in model_type and 'mobile' not in model_type:
        model_type = '_'.join(model_type.split('_')[:-1])
    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint_path)
    sam_device = comfy.model_management.get_torch_device()
    sam.to(device=sam_device)
    sam.eval()
    sam.model_name = model_file_name
    return sam

# å…¨å±€ç¼“å­˜ generatorï¼Œæé«˜æ‰¹é‡é€Ÿåº¦
_AUTO_GENERATORS = {}

# å®šä¹‰é¢œè‰²è°ƒè‰²æ¿å’Œmaskæ ‡æ³¨å™¨ï¼ˆå‚è€ƒVVL_GroundingDinoSAM2ï¼‰
COLORS_HEX = ['#FF1493', '#00BFFF', '#FF6347', '#FFD700', '#32CD32', '#8A2BE2', '#FF3838', '#FF9D97', '#FF701F', '#FFB21D', '#CFD231', '#48F90A', '#92CC17', '#3DDB86', '#1A9334', '#00D4BB', '#2C99A8', '#00C2FF', '#344593', '#6473FF', '#0018EC', '#8438FF', '#520085', '#CB38FF', '#FF95C8', '#FF37C7']
COLOR_PALETTE = sv.ColorPalette.from_hex(COLORS_HEX)
BOX_ANNOTATOR = sv.BoxAnnotator(color=COLOR_PALETTE, color_lookup=sv.ColorLookup.INDEX)
LABEL_ANNOTATOR = sv.LabelAnnotator(
    color=COLOR_PALETTE,
    color_lookup=sv.ColorLookup.INDEX,
    text_position=sv.Position.CENTER_OF_MASS,
    text_color=sv.Color.from_hex("#000000"),
    border_radius=5
)
MASK_ANNOTATOR = sv.MaskAnnotator(
    color=COLOR_PALETTE,
    color_lookup=sv.ColorLookup.INDEX
)

def annotate_image(image, detections):
    """æ ‡æ³¨å›¾åƒå‡½æ•°ï¼ˆå‚è€ƒVVL_GroundingDinoSAM2ï¼‰"""
    output_image = image.copy()
    output_image = MASK_ANNOTATOR.annotate(output_image, detections)
    output_image = BOX_ANNOTATOR.annotate(output_image, detections)
    output_image = LABEL_ANNOTATOR.annotate(output_image, detections)
    return output_image

def _get_generator(sam_model, **kwargs):
    key = id(sam_model)
    if key not in _AUTO_GENERATORS:
        _AUTO_GENERATORS[key] = SamAutomaticMaskGenerator(sam_model, **kwargs)
    return _AUTO_GENERATORS[key]

def tensor2pil(t_image: torch.Tensor) -> Image.Image:
    """è½¬æ¢tensoråˆ°PILå›¾åƒ"""
    return Image.fromarray(np.clip(255.0 * t_image.cpu().numpy(), 0, 255).astype(np.uint8))

def pil2tensor(image: Image.Image) -> torch.Tensor:
    """è½¬æ¢PILå›¾åƒåˆ°tensor"""
    return torch.from_numpy(np.array(image).astype(np.float32) / 255.0)

class VVL_SAM1Loader:
    """VVL SAM1 æ¨¡å‹åŠ è½½å™¨"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_name": (list_sam_model(), {"tooltip": "é€‰æ‹©è¦åŠ è½½çš„ SAM1 æ¨¡å‹"}),
            }
        }

    RETURN_TYPES = ("SAM_MODEL",)
    FUNCTION = "load_model"
    CATEGORY = "ğŸ’ƒrDancer/Loaders"

    def load_model(self, model_name):
        sam_model = load_sam_model(model_name)
        return (sam_model,)

class SAM1AutoEverything:
    """SAM1 AutomaticMaskGenerator ä¸€é”®åˆ†å‰²èŠ‚ç‚¹"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "sam1_model": ("SAM_MODEL", {"tooltip": "ç”± VVL_SAM1Loader åŠ è½½çš„ SAM1 æ¨¡å‹"}),
                "image": ("IMAGE", {"tooltip": "è¾“å…¥å›¾åƒï¼Œæ”¯æŒæ‰¹é‡å¤„ç†"}),
            },
            "optional": {
                "points_per_side": ("INT", {"default": 32, "min": 4, "max": 128, "tooltip": "ç”Ÿæˆç½‘æ ¼å¯†åº¦ï¼ˆç‚¹æ•°ï¼‰ã€‚å€¼è¶Šå¤§ -> mask æ›´å¤šä¸”æ›´å°ï¼›è¶Šå° -> mask æ›´å°‘ä¸”æ›´å¤§"}),
                "pred_iou_thresh": ("FLOAT", {"default": 0.86, "min": 0.0, "max": 1.0, "step": 0.01, "tooltip": "é¢„æµ‹ IoU é˜ˆå€¼ã€‚è¿‡æ»¤æ‰ç½®ä¿¡åº¦ä½çš„ maskï¼Œè¶Šé«˜è¶Šä¸¥æ ¼"}),
                "stability_score_thresh": ("FLOAT", {"default": 0.92, "min": 0.0, "max": 1.0, "step": 0.01, "tooltip": "ç¨³å®šåº¦é˜ˆå€¼ã€‚è¿‡æ»¤è½®å»“ä¸ç¨³å®šçš„ mask"}),
                "max_mask_count": ("INT", {"default": 50, "min": 1, "max": 4096, "tooltip": "æœ€ç»ˆæŒ‰é¢ç§¯æ’åºï¼Œä»…ä¿ç•™å‰ N ä¸ªæœ€å¤§ mask"}),
                "min_mask_area": ("INT", {"default": 0, "min": 0, "max": 10000, "tooltip": "è¿‡æ»¤å°äºè¯¥åƒç´ é¢ç§¯çš„ mask"}),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", "STRING", "STRING")
    RETURN_NAMES = ("annotated_image", "object_masks", "detection_json", "object_names")
    OUTPUT_IS_LIST = (False, True, False, True)
    FUNCTION = "_generate"
    CATEGORY = "ğŸ’ƒrDancer/Panoptic"

    def _generate(self, sam1_model, image: torch.Tensor, points_per_side: int = 32,
                  pred_iou_thresh: float = 0.86, stability_score_thresh: float = 0.92,
                  max_mask_count: int = 256, min_mask_area: int = 0):
        sam_model = sam1_model if not isinstance(sam1_model, dict) else sam1_model.get('model', sam1_model)
        device = sam_model.device if hasattr(sam_model, 'device') else torch.device('cpu')

        generator = _get_generator(
            sam_model,
            points_per_side=points_per_side,
            pred_iou_thresh=pred_iou_thresh,
            stability_score_thresh=stability_score_thresh,
        )

        annotated_images = []
        masks_out: List[torch.Tensor] = []
        object_names: List[str] = []
        detection_json_str = "{}"

        # ç›®å‰æŒ‰æ‰¹æ¬¡é€å¼ å¤„ç†
        for idx, img_t in enumerate(image):
            img_np = (img_t.cpu().numpy() * 255).astype(np.uint8)
            if img_np.shape[0] == 3:
                img_np = np.transpose(img_np, (1, 2, 0))
            img_pil = Image.fromarray(img_np)

            results = generator.generate(img_np)
            # æ ¹æ® area è¿‡æ»¤ã€æ’åº
            results = sorted(results, key=lambda x: x['area'], reverse=True)
            filtered = [r for r in results if r['area'] >= min_mask_area][:max_mask_count]

            bboxes, names, masks_for_detection = [], [], []
            for i, r in enumerate(filtered):
                m = torch.from_numpy(r['segmentation'].astype(np.float32))
                masks_out.append(m)
                names.append(f"mask_{i+1}")
                bboxes.append(mask_to_bbox(r['segmentation']))
                masks_for_detection.append(r['segmentation'])

            object_names = names  # æœ€ç»ˆè¿”å›æœ€åä¸€å¼ å›¾åå­—ï¼›å¤šå›¾å¯åˆå¹¶
            detection_json = build_detection_json(img_pil.width, img_pil.height, names, bboxes)
            detection_json_str = json.dumps(detection_json, ensure_ascii=False)

            # åˆ›å»ºæ ‡æ³¨å›¾åƒï¼ˆå‚è€ƒVVL_GroundingDinoSAM2çš„æ–¹æ³•ï¼‰
            if filtered and masks_for_detection:
                # åˆ›å»ºsupervisionçš„Detectionså¯¹è±¡
                xyxy_boxes = []
                for bbox in bboxes:
                    # bboxæ ¼å¼è½¬æ¢ä¸ºxyxy
                    x1, y1, x2, y2 = bbox
                    xyxy_boxes.append([x1, y1, x2, y2])
                
                xyxy_boxes = np.array(xyxy_boxes)
                masks_array = np.array(masks_for_detection)
                
                detections = sv.Detections(
                    xyxy=xyxy_boxes,
                    mask=masks_array
                )
                
                # è®¾ç½®æ ‡ç­¾
                detections.data = {'class_name': names}
                
                # æ ‡æ³¨å›¾åƒ
                annotated_img = annotate_image(img_pil, detections)
                annotated_images.append(pil2tensor(annotated_img))
            else:
                # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°å¯¹è±¡ï¼Œè¿”å›åŸå§‹å›¾åƒ
                annotated_images.append(img_t)

        # å †å æ ‡æ³¨å›¾åƒ
        annotated_images_stacked = torch.stack(annotated_images) if annotated_images else torch.empty(0)

        return (annotated_images_stacked, masks_out, detection_json_str, object_names)

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "VVL_SAM1Loader": VVL_SAM1Loader,
    "SAM1AutoEverything": SAM1AutoEverything
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "VVL_SAM1Loader": "VVL SAM1 Loader",
    "SAM1AutoEverything": "VVL SAM1 Auto Everything"
} 