import torch
import numpy as np
from PIL import Image
from typing import List, Tuple
import json
import supervision as sv  # æ·»åŠ supervisionåº“ç”¨äºå›¾åƒæ ‡æ³¨

try:
    from segment_anything import SamAutomaticMaskGenerator
except ImportError as e:
    raise ImportError("segment_anything åº“æœªå®‰è£…ï¼Œè¯·æ‰§è¡Œ pip install git+https://github.com/facebookresearch/segment-anything.git") from e

from ..utils.panoptic_utils import mask_to_bbox, build_detection_json

# å…¨å±€ç¼“å­˜ generatorï¼Œæé«˜æ‰¹é‡é€Ÿåº¦
_AUTO_GENERATORS = {}

# å®šä¹‰é¢œè‰²è°ƒè‰²æ¿å’Œmaskæ ‡æ³¨å™¨ï¼ˆå‚è€ƒVVL_GroundingDinoSAM2ï¼‰
COLORS_HEX = ['#FF1493', '#00BFFF', '#FF6347', '#FFD700', '#32CD32', '#8A2BE2', '#FF3838', '#FF9D97', '#FF701F', '#FFB21D', '#CFD231', '#48F90A', '#92CC17', '#3DDB86', '#1A9334', '#00D4BB', '#2C99A8', '#00C2FF', '#344593', '#6473FF', '#0018EC', '#8438FF', '#520085', '#CB38FF', '#FF95C8', '#FF37C7']
COLOR_PALETTE = sv.ColorPalette.from_hex(COLORS_HEX)
BOX_ANNOTATOR = sv.BoxAnnotator(color=COLOR_PALETTE, color_lookup=sv.ColorLookup.INDEX)
LABEL_ANNOTATOR = sv.LabelAnnotator(
    color=COLOR_PALETTE,
    color_lookup=sv.ColorLookup.INDEX,
    text_position=sv.Position.CENTER_OF_MASS,
    text_color=sv.Color.from_hex("#000000"),
    border_radius=5
)
MASK_ANNOTATOR = sv.MaskAnnotator(
    color=COLOR_PALETTE,
    color_lookup=sv.ColorLookup.INDEX
)

def annotate_image(image, detections):
    """æ ‡æ³¨å›¾åƒå‡½æ•°ï¼ˆå‚è€ƒVVL_GroundingDinoSAM2ï¼‰"""
    output_image = image.copy()
    output_image = MASK_ANNOTATOR.annotate(output_image, detections)
    output_image = BOX_ANNOTATOR.annotate(output_image, detections)
    output_image = LABEL_ANNOTATOR.annotate(output_image, detections)
    return output_image

def _get_generator(sam_model, **kwargs):
    key = id(sam_model)
    if key not in _AUTO_GENERATORS:
        _AUTO_GENERATORS[key] = SamAutomaticMaskGenerator(sam_model, **kwargs)
    return _AUTO_GENERATORS[key]

def tensor2pil(t_image: torch.Tensor) -> Image.Image:
    """è½¬æ¢tensoråˆ°PILå›¾åƒ"""
    return Image.fromarray(np.clip(255.0 * t_image.cpu().numpy(), 0, 255).astype(np.uint8))

def pil2tensor(image: Image.Image) -> torch.Tensor:
    """è½¬æ¢PILå›¾åƒåˆ°tensor"""
    return torch.from_numpy(np.array(image).astype(np.float32) / 255.0)

class SAM1AutoEverything:
    """SAM1 AutomaticMaskGenerator ä¸€é”®åˆ†å‰²èŠ‚ç‚¹"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "sam1_model": ("SAM_MODEL", {"tooltip": "ç”± SAMModelLoader åŠ è½½çš„ SAM1 æ¨¡å‹ï¼ˆsam_vit_h / sam_vit_l / sam_vit_b æˆ– sam_hq_vit_* ç³»åˆ—ï¼‰"}),
                "image": ("IMAGE", {"tooltip": "è¾“å…¥å›¾åƒï¼Œæ”¯æŒæ‰¹é‡å¤„ç†"}),
            },
            "optional": {
                "points_per_side": ("INT", {"default": 32, "min": 4, "max": 128, "tooltip": "ç”Ÿæˆç½‘æ ¼å¯†åº¦ï¼ˆç‚¹æ•°ï¼‰ã€‚å€¼è¶Šå¤§ -> mask æ›´å¤šä¸”æ›´å°ï¼›è¶Šå° -> mask æ›´å°‘ä¸”æ›´å¤§"}),
                "pred_iou_thresh": ("FLOAT", {"default": 0.86, "min": 0.0, "max": 1.0, "step": 0.01, "tooltip": "é¢„æµ‹ IoU é˜ˆå€¼ã€‚è¿‡æ»¤æ‰ç½®ä¿¡åº¦ä½çš„ maskï¼Œè¶Šé«˜è¶Šä¸¥æ ¼"}),
                "stability_score_thresh": ("FLOAT", {"default": 0.92, "min": 0.0, "max": 1.0, "step": 0.01, "tooltip": "ç¨³å®šåº¦é˜ˆå€¼ã€‚è¿‡æ»¤è½®å»“ä¸ç¨³å®šçš„ mask"}),
                "max_mask_count": ("INT", {"default": 50, "min": 1, "max": 4096, "tooltip": "æœ€ç»ˆæŒ‰é¢ç§¯æ’åºï¼Œä»…ä¿ç•™å‰ N ä¸ªæœ€å¤§ mask"}),
                "min_mask_area": ("INT", {"default": 0, "min": 0, "max": 10000, "tooltip": "è¿‡æ»¤å°äºè¯¥åƒç´ é¢ç§¯çš„ mask"}),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK", "STRING", "STRING")
    RETURN_NAMES = ("annotated_image", "object_masks", "detection_json", "object_names")
    OUTPUT_IS_LIST = (False, True, False, True)
    FUNCTION = "_generate"
    CATEGORY = "ğŸ’ƒrDancer/Panoptic"

    def _generate(self, sam1_model: dict, image: torch.Tensor, points_per_side: int = 32,
                  pred_iou_thresh: float = 0.86, stability_score_thresh: float = 0.92,
                  max_mask_count: int = 256, min_mask_area: int = 0):
        sam_model = sam1_model['model'] if isinstance(sam1_model, dict) else sam1_model
        device = sam_model.device if hasattr(sam_model, 'device') else torch.device('cpu')

        generator = _get_generator(
            sam_model,
            points_per_side=points_per_side,
            pred_iou_thresh=pred_iou_thresh,
            stability_score_thresh=stability_score_thresh,
        )

        annotated_images = []
        masks_out: List[torch.Tensor] = []
        object_names: List[str] = []
        detection_json_str = "{}"

        # ç›®å‰æŒ‰æ‰¹æ¬¡é€å¼ å¤„ç†
        for idx, img_t in enumerate(image):
            img_np = (img_t.cpu().numpy() * 255).astype(np.uint8)
            if img_np.shape[0] == 3:
                img_np = np.transpose(img_np, (1, 2, 0))
            img_pil = Image.fromarray(img_np)

            results = generator.generate(img_np)
            # æ ¹æ® area è¿‡æ»¤ã€æ’åº
            results = sorted(results, key=lambda x: x['area'], reverse=True)
            filtered = [r for r in results if r['area'] >= min_mask_area][:max_mask_count]

            bboxes, names, masks_for_detection = [], [], []
            for i, r in enumerate(filtered):
                m = torch.from_numpy(r['segmentation'].astype(np.float32))
                masks_out.append(m)
                names.append(f"mask_{i+1}")
                bboxes.append(mask_to_bbox(r['segmentation']))
                masks_for_detection.append(r['segmentation'])

            object_names = names  # æœ€ç»ˆè¿”å›æœ€åä¸€å¼ å›¾åå­—ï¼›å¤šå›¾å¯åˆå¹¶
            detection_json = build_detection_json(img_pil.width, img_pil.height, names, bboxes)
            detection_json_str = json.dumps(detection_json, ensure_ascii=False)

            # åˆ›å»ºæ ‡æ³¨å›¾åƒï¼ˆå‚è€ƒVVL_GroundingDinoSAM2çš„æ–¹æ³•ï¼‰
            if filtered and masks_for_detection:
                # åˆ›å»ºsupervisionçš„Detectionså¯¹è±¡
                xyxy_boxes = []
                for bbox in bboxes:
                    # bboxæ ¼å¼è½¬æ¢ä¸ºxyxy
                    x1, y1, x2, y2 = bbox
                    xyxy_boxes.append([x1, y1, x2, y2])
                
                xyxy_boxes = np.array(xyxy_boxes)
                masks_array = np.array(masks_for_detection)
                
                detections = sv.Detections(
                    xyxy=xyxy_boxes,
                    mask=masks_array
                )
                
                # è®¾ç½®æ ‡ç­¾
                detections.data = {'class_name': names}
                
                # æ ‡æ³¨å›¾åƒ
                annotated_img = annotate_image(img_pil, detections)
                annotated_images.append(pil2tensor(annotated_img))
            else:
                # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°å¯¹è±¡ï¼Œè¿”å›åŸå§‹å›¾åƒ
                annotated_images.append(img_t)

        # å †å æ ‡æ³¨å›¾åƒ
        annotated_images_stacked = torch.stack(annotated_images) if annotated_images else torch.empty(0)

        return (annotated_images_stacked, masks_out, detection_json_str, object_names)

# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {"SAM1AutoEverything": SAM1AutoEverything}
NODE_DISPLAY_NAME_MAPPINGS = {"SAM1AutoEverything": "VVL SAM1 Auto Everything"} 